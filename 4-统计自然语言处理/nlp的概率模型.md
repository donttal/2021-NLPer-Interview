# nlp的概率模型
[参考网址](https://zhuanlan.zhihu.com/p/55238865)
在自然语言处理中，概率图模型极为重要，在中文分词、词性标注、命名实体识别等诸多场景中有着广泛的应用。概率图模型（Graphical Model）分为贝叶斯网络（Bayesian Network）和马尔科夫网络（Markov Network），其中贝叶斯网络可以用一个有向图结构表示，马尔科夫网络可以用无向图结构；进一步概率图模型包含朴素贝叶斯模型、最大熵模型、隐马尔可夫模型、条件随机场、主题模型等。此外，概率图模型常常会涉及表示、推断、参数学习三大问题。
![](https://pic1.zhimg.com/v2-62c0cd56f82f872ba6a356b923706efc_r.jpg)

1、HMM VS NB，GMM

本小节从朴素贝叶斯模型（NB）和高斯混合模型（GMM）引出隐马尔可夫模型（HMM）。

1.1 朴素贝叶斯模型（NB）

NB的目标函数是后验概率最大化，其等价于0-1损失最小。NB是贝叶斯网络中最为简单的一个模型，贝叶斯网络中应熟悉概率因子分解和条件独立性判断（head to tail,tail to tail,head to head），而NB正是应用了tail to tail的条件独立性假设。


朴素贝叶斯模型（NB）
最大后验概率: [公式]

条件独立性假设: [公式]

NB的参数学习意味着需要估计 [公式] 和 [公式] ，可以采用极大似然估计（MLE）；而采用MLE可能会出现估计概率为0的情况，这会影响后验概率的计算结果，使分类产生偏差。因此，可采用贝叶斯估计解决，如add-1-smoothing 或者 add-k-smoothing。

1.2 高斯混合模型（GMM）

再引出GMM之前，首先换个角度理解NB：把NB看做单一模型，即观测变量 [公式] 由 [公式] 产生，而[公式] 也是可以观测的；一旦给定[公式]，就可以估计 [公式]。

而对于GMM，观测变量虽然 [公式]也是由[公式] 产生的，但是[公式] 确实不可以观测的。“混合”表示观测变量 [公式] 会由多个隐变量 [公式] 产生：

[公式]

GMM也是一种常见的聚类算法，使用EM算法进行迭代计算；GMM假设每个簇的分布服从高斯分布。

1.3 隐马尔可夫模型（HMM）