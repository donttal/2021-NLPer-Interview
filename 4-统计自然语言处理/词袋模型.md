# 词袋模型
[toc]

[参考网址](https://blog.csdn.net/xbinworld/article/details/90416529)
## 词向量表示
先讲一讲one-hot词向量和distributed representation分布式词向量

one hot的表示形式：word vector = [0,1,0,…,0]，其中向量维数为词典的个数|V|，当前词对应的位置为1，其他位置为0。

distributed的表示形式：word vector = [0.171,-0.589,-0.346,…,0.863]，其中向量维数需要自己指定（比如设定256维等），每个维度的数值需要通过训练学习获得。

虽然one-hot词向量构造起来很容易，但通常并不是一个好选择。一个主要的原因是，one-hot词向量无法准确表达不同词之间的相似度 

## 跳字模型（skip-gram）
![](https://img-blog.csdnimg.cn/20190526194934660.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hiaW53b3JsZA==,size_16,color_FFFFFF,t_70)

![](https://img-blog.csdnimg.cn/2019052619495378.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hiaW53b3JsZA==,size_16,color_FFFFFF,t_70)

注解：看到这里，就引出了word2vec的核心方法，其实就是认为每个词相互独立，用连乘来估计最大似然函数，求解目标函数就是最大化似然函数。上面公式涉及到一个中心词向量v，以及北京词向量u，因此呢很有趣的是，可以用一个input-hidden-output的三层神经网络来建模上面的skip-model。

Skip-gram可以表示为由输入层（Input）、映射层（Projection）和输出层（Output）组成的神经网络（示意图如下，来自[2]）：
![](https://img-blog.csdnimg.cn/20190526195633208.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hiaW53b3JsZA==,size_16,color_FFFFFF,t_70)


输入的表示：输入层中每个词由独热编码方式表示，即所有词均表示成一个N维向量，其中N为词汇表中单词的总数。在向量中，每个词都将与之对应的维度置为1,其余维度的值均为0。
网络中传播的前向过程：输出层向量的值可以通过隐含层向量（K维），以及连接隐藏层和输出层之间的KxN维权重矩阵计算得到。输出层也是一个N维向量，每维与词汇表中的一个单词相对应。最后对输出层向量应用Softmax激活函数，可以计算每一个单词的生成概率。

## 连续词袋模型（CBOW）
CBOW就倒过来，用多个背景词来预测一个中心词，CBOW对小型数据库比较合适，而Skip-Gram在大型语料中表现更好[2]。但是方法和上面是很像的，因此这里我就放下图。推导的方法是一样的。

![](https://img-blog.csdnimg.cn/20190526202244935.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hiaW53b3JsZA==,size_16,color_FFFFFF,t_70)

![](https://img-blog.csdnimg.cn/20190526202310937.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hiaW53b3JsZA==,size_16,color_FFFFFF,t_70)