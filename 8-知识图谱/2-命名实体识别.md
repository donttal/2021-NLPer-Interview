## 2. 实体识别
[toc]

### 2.1 简介

命名实体识别（Named Entity Recognition，简称 NER），是指识别文本中具有特定含义的实体，常用 NER 数据集中的实体类型主要包括人名、地名、机构名、专有名词等，以及时间、数量、货币、比例数值等文字。命名实体指的是可以用专有名词标识的事物，一个命名实体一般代表唯一一个具体事物个体，包括人名、地名等。

### 2.2 数据集和评测指标

常用的中文 NER 数据集包括，OntoNotes4.0 [12]，MSRA [13] 和 Weibo [14] 等，前两个是由新闻文本中抽取得到，后一个是由社交媒体中抽取得到。常用的英文数据集有 CoNLL2003 [15]，ACE 2004 [16] 和 OntoNotes 5.0 [17] 等。想了解更多数据集，建议参见 [74]。

在数据标注上，主要有 BIO（Beginning、Inside、Outside）和 BIOES（Beginning、Inside、End、Outside、Single）两种标注体系。此外，还有针对复杂实体抽取建立的改进版本的标注方法，将会在 2.4.4 部分进行介绍。

在模型评测上，由于命名实体的识别包括实体边界和类型的识别，因此只有一个实体的边界和类型都被正确识别时，才能被认为实体被正确识别。根据对实体边界预测的精准度的要求不同可以分为 Exact Match 或 Relaxed Match，并且使用准确率，召回率以及 F1 值来计算得分。目前，基于 Exact Match 的 micro 的准确率，召回率以及 F1 值最为常用。

### 2.3 面临的挑战

目前，命名实体识别在行业知识图谱构建方面主要面临如下挑战：

垂直领域标注语料少，导致模型效果不好
垂直领域细分类别很多，在进入一个新的垂直领域时，往往可用的监督数据是很有限的。在此基础上所训练得到的模型的识别效果是不尽人意的。
垂直领域先验知识未能有效利用
在有监督数据足够的前提下，行业内其他类型的先验知识的量相对来讲是更大的。但是这些行业数据却没有很合理的应用到 NER 任务中来更有效的提升模型性能。
垂直领域复杂实体难以识别
一般研究和落地中遇到的实体识别大多为连续实体的识别，但复杂实体识别在实际应用中的占比越来越高，特别是在医疗领域的实体抽取中。

### 2.4 主流NER深度学习模型

我们对和前述挑战相关的技术进展进行了调研，本节内容给出相应的汇报和分析。

#### 2.4.1 经典模型

基于深度学习的 NER 模型，大都将 NER 任务建模为序列标注任务，并且以 Encoder-Decoder 架构来进行建模。

最先将深度学习应用于 NER 任务的模型当数 LSTM+CRF 模型 [20]，不同于经典的人工特征设计，LSTM+CRF 模型基于数据来进行特征学习，且取得了很好的效果，极大推进了深度学习在 NER 中应用的进程。

之后，在模型设计中，BiLSTM [21] 取代了 LSTM 作为Encoder。除了以 LSTM 为代表的循环神经网络RNN模型作为 Encoder，也有以卷积神经网络 CNN 作为 Encoder 的实践。

较新的，ID-CNNs [22] 利用 dilated CNN 模型（见下面示意图）解决了原本 CNN 感受野随着卷积层数的线性增长性的局限性，从而扩大了 Encoder 的感受野，进而能整合与利用更加长程的信息进行预测。

以 BERT [23] 为代表的预训练语言模型的出现，使得以 BERT 作为 Encoder 成为新的最强 Baseline，在应用落地中，往往借助知识蒸馏的技术来对 BERT 模型进行蒸馏，从而提升在线预测的效率。

#### 2.4.2 知识增强的模型

（1）词汇增强

对于中文任务来说，句子中的词汇信息显然是重要的，但是先对句子进行分词，在词序列的基础上进行序列标注任务，这种 NER 模型架构的效果受限于分词的准确性。因此，如何将句子中的词汇信息合理的整合到基于字的序列标注模型中，是中文 NER 主流研究方向之一。

Lattice-LSTM [24] 将句子表示为由其中的词汇和字构成的 Lattice 结构（见下图）。在基于字序列的 LSTM 基础上，Lattice-LSTM 仿效 LSTM 的信息传递机制，将词汇的信息整合进该词汇的首尾字符的表示中。如此模型便将字符级信息和词汇级信息进行了有机的融合，既丰富了模型的语义表达，又使得模型对分词带来的噪声有很好的鲁棒性。

在中文数据集 MSRA [13] 和 WeiBo [14] 上，Lattice-LSTM 的 F1 值相较于基于字符和基于词汇的模型的最好性能均有 2% 以上的性能提升。
![](https://pic2.zhimg.com/v2-c88f68b5301197ba4746fbc9dbc50725_r.jpg)

LR-CNN [25] 模型通过利用 CNN 模型，以及在 CNN 中引入 Rethink 机制来解决 Lattice-LSTM 模型不能并行化以及句子中词汇之间的混淆的问题。具体的，LR-CNN 将不同 layer 的卷积结果看作不同 n-gram 字符组的向量表示，再将句子中中的词汇向量以 attention 的方式整合到其对应的 n-gram 字符组的向量表示中，以此来整合词汇信息。

为了解决词汇混淆的问题，LR-CNN 将 CNN 的最后一层的 feature 向量和 CNN 每一层的向量表示再次进行attention，从而达到利用最后一层的 feature 来调优 前面特征筛选和表达的效果，进而能够使得模型自适应的调节词汇之间的混淆。

在中文数据集 MSRA [13] 和 WeiBo [14] 上，LR-CNN 相较于 Lattice-LSTM 的 F1 值分别有 0.6% 和 1.2% 的性能提升。

![](https://pic3.zhimg.com/v2-cc595adbb95888eb37d57334cb36c0fa_r.jpg)
FLAT [26] 在融合字符与词汇的 Lattice 结构上，引入 Transformer 来进行建模。相对于上面以 RNN 和 CNN 为基础架构的模型，FLAT 能整合更加长程的信息的同时，还能更充分的利用 GPU 资源进行并行化训练和推理。

其主要模型点在于：一、将 Lattice 结构按照字符的位置以及词汇的头尾字符的位置重构为序列结构；二、由于 Transformer 所利用的绝对位置向量编码无法很好的建模序列中的顺序信息，因此，FLAT 根据词汇之间的头尾，头头，尾头，尾尾字符距离定义了四种距离，并且对这四种距离进行向量编码。

考虑字符/词汇与其他字符/词汇的向量表示，以及距离的向量表示进行权重计算，最终得到相应的 attention。在中文数据集 MSRA [13] 和 WeiBo [14] 上，FLAT 相较于 LR-CNN 的 F1 值分别有 0.6% 和 3% 的性能提升。

![](https://pic2.zhimg.com/v2-14397e65af54dfb52e55f10d2303159d_r.jpg)

（2）实体类型信息增强

BERT-MRC [27] 将所要预测的实体类型的描述信息作为先验知识输入到模型中，并且将 NER 问题建模为阅读理解问题（MRC），最终通过 BERT 来进行建模。

具体的，给定句子 S 和所要抽取的实体类型如“organization”，其通过问句生成模块将“organization”转换为问句Q“find organizations including companies, agencies and institutions”，将此 Q 和 S 作为两个句子输入到 BERT 中进行训练。

由于实体类型先验知识的加入，在中文数据集 OntoNotes4.0 一半训练数据的基础上，BERT-MRC 的模型效果就能达到单纯将句子 S 输入到 BERT 进行序列标注的模型在全量数据上训练的效果。

此外，由于把每类数据的识别进行了区分，因此，此类模型能有效的解决复杂实体识别中的实体交叉和嵌套问题（见2.4.4）。在中文数据集 MSRA [13] 上，BERT-MRC 相较于前述 FLAT 模型有 1.4% 的提升，达到 95.75% 的 F1 值。

TriggerNER [28] 同样是将实体的类型信息作为模型的输入的一部分，区别于 BERT-MRC，其实体类型信息来源于句子中的一部分词汇，称为 Trigger words。如下图例子所示，通过句子中蓝色字体的 Trigger 词汇，可以推断出 Rumble Fish是一个餐馆名称。在模型实现上，TriggerNER分为TriggerEncoder&Matcher 和 Trigger-Enhanced Sequence Tagging 两部分，此两部分都是基于同一个 BiLSTM 提供词汇的表示信息。

TriggerEncoder&Matcher 部分主要在于基于 Trigger 的表示进行实体类型的预测以及原句子表示与 Trigger 词汇序列表示的匹配，Trigger-Enhance 部分将 BiLSTM 提供的表示信息与 Trigger Encoding 提供的表示信息进行整合，最终通过 CRF 层进行模型输出。

在预测阶段，测试集中句子的 Trigger 词汇是来自于在训练集中整理得到的 Trigger 词典匹配得来。在 CONLL2003 英文数据集上，TriggerNER 在 20% 训练集上进行 Trigger 标注后训练得到的效果和 BiLSTM-CRF 在 70% 原始训练集上训练得到的效果相当。

![](https://pic3.zhimg.com/v2-e2fe71366031fbf6f63e4dd7a12099ea_r.jpg)

#### 2.4.3 半监督模型

半监督算法旨在在有标签和无标签的数据集上对模型进行建模（整体模型分类见下图）。利用无标记数据进行神经网络半监督学习，在 NER 领域中得到了广泛的研究。
![](https://pic1.zhimg.com/v2-3770f1d5c4800e69d06b881c2a683cc0_r.jpg)

以 BERT [23] 为代表的预训练语言模型，基于大规模的无标签数据，利用 random mask 等机制对词序列的联合概率分布进行建模，从而进行自监督训练，最终能够很好的将文本知识整合到词向量的表示中。在此基础上，在有标签的数据上进行 fine-tune，即可得到效果不错的 NER 模型。

NCRF-AE [29] 将 label 信息建模为隐变量，进而利用 autoencoder 的模型来同时对有标签和无标签数据进行建模训练。具体来说，通过将 label 信息建模为隐变量y, 进而将原本需要预测的概率分布 P(y|x) 替换为如下带隐变量的 encoder-decoder 模型，进而可以利用无标签数据的重构损失来增强标签信息的建模。
![](https://pic1.zhimg.com/v2-b6d20c6a1b9ac777203c894997baa4a0_r.jpg)

区别于 NCRF-AE 将标签信息直接建模为隐变量的方式，VSL-G [30] 通过引入纯粹的隐变量及隐变量之间的层次化结构，并且利用 variational lower bound 来构建重构损失函数，从而将有监督损失和无监督损失函数独立开来。此模型的重要意义在于引入并设计了隐变量之间的层次化结构，在此基础上引入的 VAE 下界损失对于有监督模型中参数起到了很好的正则化作用，从而达到了在小型数据集上就训练就有很好的泛化性能。

将一个语种中的句子 A 翻译成另一个语种的句子 B，再将其翻译回来 C，从而得到（A, C）平行语料。LADA [31] 发现 A 和 C 中大都包含相同数目的目标类别实体。基于此发现，LADA 将模型在无标签句子 A，C 的每个 token 上的输出向量进行加和，得到的向量为该句子所包含的每类实体的数目向量，将此两个向量的差值的 l2_ 范数作为在无监督样本上的损失。从而可以利用大规模的无监督数据进行模型训练，在数据量较少的情况下，达到了提升模型准确率的效果。

更多的，LADA [31] 将图像领域中用于数据增强的 Mixup 方法引入到 NER 中来。Mixup 方法的核心在于对特征向量进行插值，从而得到新的训练数据。由于 NER 属于序列标注问题，因此需要合理的设计多个 token 的的隐向量的插值方式。LADA [31] 采用将原句子 token 序列进行重新排列组合以及对训练句子集进行 KNN 聚类的方式，得到了句内和句间两种插值方式，实验证明这种插值方式在 NER 上是有效果的。

![](https://pic3.zhimg.com/v2-3d90967588f5bab3da77e500a8aa9b26_r.jpg)

相比于 LADA 在隐向量层面进行数据增强，ENS-NER [32] 模型采用在词向量上添加高斯噪声的统计学数据增强手段，以及随机掩盖 token 和同义词替换的语言学数据增强手段，从而达到数据增强效果。在相关数据集上的实验证实此类数据增强对于 NER 是有增益的，而且语言学数据增强和统计学数据增强手段的效果相当的。

值得注意的是，除 BERT 等语言模型之外，以上几类半监督模型在原有标签数据量占原有训练集较小比例时，如 10% 左右，其效果是明显的，但是当原有标签训练数据占比变大时，非原有标签数据给模型带来的增益并不明显。

#### 2.4.4 复杂实体

前述模型主要针对连续实体的抽取进行建模，在实际应用中还存在部分复杂实体的识别问题。这里的复杂指的是存在不连续的单实体以及多实体之间的覆盖和交叉关系。下图分别给出不连续实体（discontinuous entity），嵌套实体（nested entities）和 交叉实体（overlapping entities）的例子。


[33] 为解决含有不连续实体的 overlapping 实体识别问题，引入了 BIO 标注体系的变体，即在 BIO 的基础上，增加了 BD，BI，BH，IH 四个指标，分别代表Beginning of Discontinuous body, Inside of Discontinuous body, Beginning of Head 和 Inside of Head。以上面图 c 为例，在新的标注体系下，标注结果为：肌（BH）肉（IH）疼（B）痛（I）和（O）疲（BD）劳（ID）。此类方法的缺陷在于，如果同一句子中出现多个不连续的实体，则会出现实体混淆问题。

![](https://pic2.zhimg.com/v2-d61a0e73fbde38b9bf13c60a5d8ce489_r.jpg)

[33] 为解决含有不连续实体的 overlapping 实体识别问题，引入了 BIO 标注体系的变体，即在 BIO 的基础上，增加了 BD，BI，BH，IH 四个指标，分别代表Beginning of Discontinuous body, Inside of Discontinuous body, Beginning of Head 和 Inside of Head。以上面图 c 为例，在新的标注体系下，标注结果为：肌（BH）肉（IH）疼（B）痛（I）和（O）疲（BD）劳（ID）。此类方法的缺陷在于，如果同一句子中出现多个不连续的实体，则会出现实体混淆问题。

[34] 基于 transition-based 方法，引入更加丰富的 action 类别来解决不连续实体 overlapping 识别的问题。具体的，其使用 stack 存储处理过的 span，并使用 buffer 存储未处理的 token。NER 可以重塑为如下过程：给定解析器的状态，预测一个用于更改解析器状态的 action，重复此过程，直到解析器达到结束状态（即 stack 和 buffer 均为空）为止（图下图所示）。显然，此类方法不仅能解决不连续实体识别，也能解决实体嵌套和部分重叠，因此尽管此类方法相较于前述标注方法设计更加复杂，但其给出了解决连续和复杂实体识别的统一框架。此外，此方法属于序列决策问题，因而一个可能的方向是利用深度强化学习的方法来重塑目标函数和优化过程。

![](https://pic4.zhimg.com/v2-9faded1dab0e99ce1193d06da1bbe467_r.jpg)

更多地，[35] 引入句子的 hypergraph 结构表示来解决多类别实体嵌套和不连续识别问题，相较于经典模型的序列预测，其以局部子图的预测为最终目标。

### 2.5 小结

本节围绕实体识别任务所面临的三个挑战：标注数据少，行业知识未充分利用以及复杂实体难抽取，对相关技术进展进行介绍，主要包括以 Bi-LSTM+CRF 为代表的经典模型，知识增强的模型，半监督模型和复杂实体识别模型。

从实际应用来看，在经典模型的基础上结合行业词典或实体关系描述的方法得到了广泛的应用，但是在复杂实体的识别上，目前还没有很好的模型结构或者简洁有效的解决方案。