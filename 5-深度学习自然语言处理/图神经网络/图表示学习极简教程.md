# 图表示学习极简教程
[参考网址](https://mp.weixin.qq.com/s/oxC8prvqN1h5Zr4nSlQl_Q)
## Introduction
随着 Deep Learning 的爆火，图数据挖掘和 CV、NLP 等领域一样，存在着“爆发式”发展的趋势。更加准确地说，笔者认为图数据挖掘正处在爆发的前夜。本文主要从**基于图结构的表示学习**和**基于图特征的表示学习**两个角度简要介绍图表示学习的现状和自己的认识。

在非图的表示学习中，研究者们主要考虑的是每一个研究对象的特征(姓名、年龄、身高等)信息。然而，研究对象是存在于客观世界的主体，存在一定的图结构信息（QQ、微信好友，师生关系等都构成了图网络）。如何对图结构进行表示学习以表示图的结构信息是一个很重要的 topic。

图表示学习的主要目标是：将结点映射为向量表示的时候尽可能多地保留图的拓扑信息。**图表示学习主要分为基于图结构的表示学习和基于图特征的表示学习**。

如图1，基于图结构的表示学习对结点的向量表示只来源于图的拓扑结构（ nxn  的邻接矩阵表达的图结构），只是对图结构的单一表示，缺乏对图结点特征消息的表示。
如图2，基于图特征的表示学习对结点的向量表示既包含了图的拓扑信息（ nxn  的邻接矩阵表达的图结构）也包含了已有的特征向量（ 个维度为  包含结点特征的向量，如姓名、年龄、身高等信息）。

[](https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGdzupgpu9Xs5tia8SNBCMAZTAzcyx3B29jia8iaexez3Kqtibpy4EYQEibbIibFcA0Dia2jhRZmV64OYmiaQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

通过上述的介绍，我们可以知道图表示学习的 task 就是用 n个向量表示图上的 n个结点，这样我们就可以将一个难以表达的拓扑结构转化为可以丢进炼丹炉的 vector 啦。

## 基于图结构的表示学习

在我们的图表示学习中，我们希望 Embedding 出来的向量在图上“接近”时在向量空间也“接近”。对于第 2 个“接近”，就是欧式空间两个向量的距离。对于第一个“接近”，可以有很多的解释：

* 1-hop：两个相邻的结点就可以定义为临近；
* k-hop：两个k阶临近的结点也可以定义为临近；
* 具有结构性：结构性相对于异质性而言。异质性关注的是结点的邻接关系；结构性将两个结构上相似的结点定义为“临近”。比方说，某两个点是各自小组的中心，这样的两个节点就具有结构性。
因此，针对上述的一些观点，就有了下列的模型：

### 2.1 DeepWalk
DeepWalk [1] 的方法采用了 Random walk 的思想进行结点采样。

具体参见图 3，我们首先根据用户的行为构建出一个图网络；随后通过 Random walk 随机采样的方式构建出结点序列（例如：一开始在 A 结点，A->B，B 又跳到了它的邻居结点 E，最后到 F，得到"A->B->E->F"序列）；对于序列的问题就是 NLP 中的语言模型，因为我们的句子就是单词构成的序列。

接下来我们的问题就变成 Word2vec（词用向量表示）的问题，我们可以采用 Skip-gram [2] 的模型来得到最终的结点向量。可以说这种想法确实是十分精妙，将图结构转化为序列问题确实是非常创新的出发点。

在这里，结点走向其邻居结点的概率是均等的。当然，在有向图和无向图中，游走的方式也不一样。无向图中的游走方式为相连即可走；而有向图中则是只沿着“出边”的方向走。

![](https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGdzupgpu9Xs5tia8SNBCMAZqiczI0toYKRLNegznYyC2XCbeQ973Sibnibuyykf5VL2OXblemL82Qia5w/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

▲ 图3：DeepWalk（图源：阿里的paper）

### 2.2 Node2vec
之前所述的 Random Walk 方法中，一个结点向邻居结点游走的概率是相等的。这种等概率的游走操作似乎是比较 naive 的，对此，Node2vec [3] 的提出就是对结点间游走概率的定义。在图 4 中，我们看到当从结点 t 跳跃到结点 v 之后，算法下一步从结点 v 向邻居结点跳跃的概率是不同的。

![](https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGdzupgpu9Xs5tia8SNBCMAZDUAdpEvg9caEQInzVXAicstuCDE0gkxZiaLJxaYUzUsjGpOrO7dwqstQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

▲ 图4：Node2vec结点的跳转概率示意
具体的跳转概率（这里的“概率”不是严格的概率，实际上要对下面这个公式进行归一化处理）定义为：
![](https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGdzupgpu9Xs5tia8SNBCMAZhxUV5fGqPygDYvpToLzicWiaPzCico4OeTs0C58ae01BLpKJp90iaULspw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

该公式中 $W_{vx}$ 后项表示权重，$$ 定义如下：

在上面的公式中，从结点 v 回跳到上一个结点 t 的  为 ；从结点 v 跳到 t、v 的公共邻居结点的 为 1；从结点v跳到其他邻居的  为 。
根据上述的方法，我们就可以获得节点间的跳转概率了。我们发现，当 p 比较小的时候，结点间的跳转类似于 BFS，结点间的“接近”就可以理解为结点在邻接关系上“接近”；当 q 比较小的时候，结点间的跳转类似于 DFS，节点间的“接近”就可以视作是结构上相似。具体可借助图 5 理解。

![](https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGdzupgpu9Xs5tia8SNBCMAZcYrNibVROqic8TqcgqpStiaBu22uricBG1ZfOibzjD0q0tiaEBzLpw4eLk5w/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

▲ 图5：p、q取值不同时结点的游走趋势


### 2.3 Metapath2vec, LINE and so on

针对异构图而言，其结点的属性不同，采样的方式也与传统的图网络不同，需要按照定义的 Meta-Path [4] 规则进行采样。采样的样例可类似于“电影-导演-主演”这样的方法进行采样。

对于唐建等人提出的 LINE [5] 中，他们认为 1-hop 和 2-hop 临近的结点为“接近”的结点。关于 Embedding 的技术还有很多，这里就不作详述啦。


## 基于图特征的表示学习
在基于图特征的表示学习中，由于加入了结点的特征矩阵 （姓名、年龄、身高等这样的特征），需要同基于图结构的表示学习区别开来。这一类的模型通常被叫做“图神经网络”。

### 3.1 GCN

Graph Convolutional Networks [6]（图卷积网络）是非常基本第一个 GNN 模型。在讨论 GCN 之前，我们先来看一下 CNN（卷积神经网络）是怎么做卷积运算的。如图 6 所示，CNN 的两个主要特点是局部感知与权值共享。换句话说，**就是聚合某个像素点周围的像素特征**。

类似地，在 GCN 中，新的特征也是对上一层邻居结点特征的聚合，公式如下：

$H^{(l)}=\sigma\left(D^{-\frac{1}{2}}(A+I) D^{-\frac{1}{2}} H^{(l-1)} W^{l}\right)$
