# 对话系统知识概要
[参考网址](http://www.shareditor.com/bloglistbytag/?tagname=%E8%87%AA%E5%B7%B1%E5%8A%A8%E6%89%8B%E5%81%9A%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA)
[全套代码](https://github.com/warmheartli/ChatBotCourse)
## 1. 自动词性标注
**默认标注器：**不管什么词，都标注为频率最高的一种词性。比如经过分析，所有中文语料里的词是名次的概率是13%最大，那么我们的默认标注器就全部标注为名次。这种标注器一般作为其他标注器处理之后的最后一道门
**正则表达式标注器：**满足特定正则表达式的认为是某种词性，比如凡是带“们”的都认为是代词(PRO)
**查询标注器：**找出最频繁的n个词以及它的词性，然后用这个信息去查找语料库，匹配的就标记上，剩余的词使用默认标注器(回退)。一般使用一元标注的方式
一元标注：基于已经标注的语料库做训练，然后用训练好的模型来标注新的语料
二元标注和多元标注：一元标注指的是只考虑当前这个词，不考虑上下文。二元标注器指的是考虑它前面的词的标注，用法只需要把上面的UnigramTagger换成BigramTagger。同理三元标注换成TrigramTagger
## 分块
分块就是根据句子中的词和词性，按照某种规则组合在一起形成一个个分块，每个分块代表一个实体。常见的实体包括：组织、人员、地点、日期、时间等
以“我下午要和小明在公司讨论一个技术问题”为例，首先我们做名词短语分块（NP-chunking），比如：技术问题。名词短语分块通过词性标记和一些规则就可以识别出来，也可以通过机器学习的方法识别
除了名词短语分块还有很多其他分块：介词短语（PP，比如：以我……）、动词短语（VP，比如：打人）、句子（S，我是人）
### 分块的存储
可以采用IOB标记，I(inside，内部)、O(outside，外部)、B(begin, 开始)，一个块的开始标记为B，块内的标识符序列标注为I，所有其他标识符标注为O
也可以用树结构来存储分块，用树结构可以解决IOB无法标注的另一类分块，那就是多级分块。多级分块就是一句话可以有多重分块方法，比如：我以我的最高权利惩罚你。这里面“最高权利”、“我的最高权利”、“以我的最高权利”是不同类型分块形成一种多级分块，这是无法通过IOB标记的，但是用树结构可以。这也叫做级联分块。具体树结构举个例子：
```
(S
    (NP 小明
    （VP
        （V 追赶
        （NP 
            （Det 一只
            （N 兔子））））
```
## 聊天机器人
提问处理模快，检索模块，答案抽取模块
### 提问处理模块
**查询关键词生成、答案类型确定、句法和语义分析**
查询关键词生成，就是从你的提问中提取出关键的几个关键词，因为我本身是一个空壳子，需要去网上查找资料才能回答你，而但网上资料那么多，我该查哪些呢？所以你的提问就有用啦，我找几个中心词，再关联出几个扩展词，上网一搜，一大批资料就来啦，当然这些都是原始资料，我后面要继续处理。
再说答案类型确定，这项工作是为了确定你的提问属于哪一类的，如果你问的是时间、地点，和你问的是技术方案，那我后面要做的处理是不一样的。
最后再说这个句法和语义分析，这是对你问题的深层含义做一个剖析，比如你的问题是：聊天机器人怎么做？那么我要知道你要问的是聊天机器人的研发方法
### 检索模块
跟搜索引擎比较像，就是根据查询关键词所信息检索，返回句子或段落，这部分就是下一步要处理的原料
### 答案抽取模块
可以说是计算量最大的部分了，它要通过分析和推理从检索出的句子或段落里抽取出和提问一致的实体，再根据概率最大对候选答案排序，注意这里是“候选答案”噢，也就是很难给出一个完全正确的结果，很有可能给出多个结果，最后还在再选出一个来
## 关键技术
1）海量文本知识表示：网络文本资源获取、机器学习方法、大规模语义计算和推理、知识表示体系、知识库构建；

2）问句解析：中文分词、词性标注、实体标注、概念类别标注、句法分析、语义分析、逻辑结构标注、指代消解、关联关系标注、问句分类（简单问句还是复杂问句、实体型还是段落型还是篇章级问题）、答案类别确定；

3）答案生成与过滤：候选答案抽取、关系推演（并列关系还是递进关系还是因果关系）、吻合程度判断、噪声过滤

## 问句解析
中文NLP工具
第一个要数哈工大的LTP(语言技术平台)了，它可以做中文分词、词性标注、命名实体识别、依存句法分析、语义角色标注等丰富、 高效、精准的自然语言处理技术

第二个就是博森科技了，它除了做中文分词、词性标注、命名实体识别、依存文法之外还可以做情感分析、关键词提取、新闻分类、语义联想、时间转换、新闻摘要等，但因为是商业化的公司，除了分词和词性标注免费之外全都收费

第三个就是jieba分词，这个开源小工具分词和词性标注做的挺不错的，但是其他方面还欠缺一下，如果只是中文分词的需求完全可以满足

第四个就是中科院张华平博士的NLPIR汉语分词系统，也能支持关键词提取

推荐的是NLPIR

## 依存句法分析
依存句法分析的基本任务是确定句式的句法结构(短语结构)或句子中词汇之间的依存关系。依存句法分析最重要的两棵树：

依存树：子节点依存于父节点

依存投射树：实线表示依存联结关系，位置低的成分依存于位置高的成分，虚线为投射线
![](http://shareditor-shareditor.oss-cn-beijing.aliyuncs.com/dynamic/db40ccb18b08d48b803618276d98d08b9ff07265.png)

### 五条公理
1. 一个句子中只有一个成分是独立的
2. 其他成分直接依存于某一成分
3. 任何一个成分都不能依存于两个或两个以上的成分
4. 如果A成分直接依存于B成分，而C成分在句子中位于A和B之间，那么C或者直接依存于B，或者直接依存于A和B之间的某一成分
5. 中心成分左右两面的其他成分相互不发生关系
![](https://cdn.jsdelivr.net/gh/donttal/figurebed/img/LTP依存关系标记.png)

## 中文分词
1）歧义消除；2）未登陆词识别
### n元语法模型
N-最短路径分词法其实就是一元语法模型，每个词成为一元，独立存在，出现的概率可以基于大量语料统计得出，比如“确实”这个词出现概率的0.001（当然这是假设，别当真），我们把一句话基于词表的各种切词结果都列出来，因为字字组合可能有很多种，所以有多个候选结果，这时我们利用每个词出现的概率相乘起来，得到的最终结果，谁最大谁就最有可能是正确的，这就是N-最短路径分词法。

这里的N的意思是说我们计算概率的时候最多只考虑前N个词，因为一个句子可能很长很长，词离得远，相关性就没有那么强了

这里的最短路径其实是传统最短路径的一种延伸，由加权延伸到了概率乘积

而基于n元语法模型的分词法就是在N-最短路径分词法基础上把一元模型扩展成n元模型，也就是统计出的概率不再是一个词的概率，而是基于前面n个词的条件概率

### 基于词分词
每个字在词语中都有一个构词位置：词首、词中、词尾、单独构词。根据一个字属于不同的构词位置，我们设计出来一系列特征，比如：前一个词、前两个词、前面词长度、前面词词首、前面词词尾、前面词词尾加上当前的字组成的词……

我们基于大量语料库，利用平均感知机分类器对上面特征做打分，并训练权重系数，这样得出的模型就可以用来分词了，句子右边多出来一个字，用模型计算这些特征的加权得分，得分最高的就是正确的分词方法

## 概率图模型
概率图模型一般是用图来说明，用概率来计算的。

分成有向图模型和无向图模型，顾名思义，就是图里面的边是否有方向。那么什么样的模型的边有方向，而什么样的没方向呢？这个很好想到，有方向的表达的是一种推演关系，也就是在A的前提下出现了B，这种模型又叫做生成式模型。而没有方向表达的是一种“这样就对了”的关系，也就是A和B同时存在就对了，这种模型又叫做判别式模型。生成式模型一般用联合概率计算(因为我们知道A的前提了，可以算联合概率)，判别式模型一般用条件概率计算(因为我们不知道前提，所以只能"假设"A条件下B的概率)。生成式模型的代表是：n元语法模型、隐马尔可夫模型、朴素贝叶斯模型等。判别式模型的代表是：最大熵模型、支持向量机、条件随机场、感知机模型等
### 常用概念
贝叶斯(Bayes)：无论什么理论什么模型，只要一提到他，那么里面一定是基于条件概率P(B|A)来做文章的。ps：贝叶斯老爷爷可是18世纪的人物，他的理论到现在还这么火，可见他的影响力绝不下于牛顿、爱因斯坦

马尔可夫(Markov)：无论什么理论什么模型，只要一提到他，那么里面一定有一条链式结构或过程，前n个值决定当前这个值，或者说当前这个值跟前n个值有关

熵(entropy)：熵有火字旁，本来是一个热力学术语，表示物质系统的混乱状态。延伸数学上表达的是一种不确定性。延伸到信息论上是如今计算机网络信息传输的基础理论，不确定性函数是f(p)=-logp，信息熵H(p)=-∑plogp。提到熵必须要提到信息论鼻祖香农(Shannon)

场(field)：只要在数学里见到场，它都是英文里的“域”的概念，也就是取值空间，如果说“随机场”，那么就表示一个随机变量能够赋值的全体空间
### 马尔科夫模型和隐马尔科夫模型
马尔科夫明显特征就是一个值跟前面的n个值有关，也就是条件概率，生成式模型
马尔可夫模型还可以看成是一个关于时间t的状态转换过程，也就是随机的有限状态机，那么状态序列的概率可以通过计算形成该序列所有状态之间转移弧上的概率乘积得出。
#### 隐马尔科夫和马尔科夫的区别
这里的“隐”指的是其中某一阶的信息我们不知道，就像是我们知道人的祖先是三叶虫，但是由三叶虫经历了怎样的演变过程才演变到人的样子我们是不知道的，我们只能通过化石资料了解分布信息，如果这类资料很多，那么就可以利用隐马尔可夫模型来建模，因为缺少的信息较多，所以这一模型的算法比较复杂，比如前向算法、后向算法之类晦涩的东西就不说了。相对于原理，我们更关注它的应用，隐马尔可夫模型广泛应用在词性标注、中文分词等，为什么能用在这两个应用上呢？仔细想一下能看得出来，比如中文分词，最初你是不知道怎么分词的，前面的词分出来了，你才之后后面的边界在哪里，但是当你后面做了分词之后还要验证前面的分词是否正确，**这样前后有依赖关系，而不确定中间状态的情况最适合用隐马尔可夫模型来解释**

## 命名实体识别
### 条件随机场
适用于在一定观测值条件下决定的随机变量有限个取值的情况。给定观察序列X时某个特定的标记序列Y的概率是一个指数函数exp(∑λt+∑μs)，这也正符合最大熵原理。基于条件随机场的命名实体识别方法属于有监督的学习方法，需要利用已经标注好的大规模语料库进行训练。
### 命名实体的放射性
举个栗子：“中国积极参与亚太经合组织的活动”，这里面的“亚太经合组织”是一个命名实体，定睛一瞧，这个实体着实不凡啊，有“组织”两个字，这么说来这个实体是一种组织或机构，记住，下一次当你看到“组织”的时候和前面几个字组成的一定是一个命名实体。继续观察，在它之前辐射出了“参与”一次，经过大规模语料训练后能发现，才“参与”后面有较大概率跟着一个命名实体。继续观察，在它之后有“的活动”，那么说明前面很可能是一个组织者，组织者多半是一个命名实体。这就是基于条件随机场做命名实体识别的奥秘，这就是命名实体的放射性

## 词性自动标注
分词，命名实体识别和词性标注
常说的词性包括：名、动、形、数、量、代、副、介、连、助、叹、拟声。但自然语言处理中要分辨的词性要更多更精细，比如：区别词、方位词、成语、习用语、机构团体、时间词等，多达100多种。

汉语词性标注最大的困难是“兼类”，也就是一个词在不同语境中有不同的词性，而且很难从形式上识别。
### 基于统计模型
基于统计模型，势必意味着我们要利用大量已经标注好的语料库来做训练，同时要先选择一个合适的训练用的数学模型，隐马尔科夫模型(HMM)比较适合词性标注这种基于观察序列来做标注的情形。

#### 隐马尔科夫模型参数初始化的技巧
HMM是一种基于条件概率的生成式模型，所以模型参数是生成概率，那么我们不妨就假设每个词的生成概率就是它所有可能的词性个数的倒数，这个是计算最简单又最有可能接近最优解的生成概率了。每个词的所有可能的词性是我们已经有的词表里标记好的，这个词表的生成方法就比较简单了，我们不是有已经标注好的语料库嘛，很好统计。那么如果某个词在词表里没有呢？这时我们可以把它的生成概率初值设置为0。这就是隐马尔可夫模型参数初始化的技巧。

### 统计方法和规则方法结合的词性标注
统计方法覆盖面比较广，新词老词通吃，常规非常规通吃，但对兼词、歧义等总是用经验判断，效果不好。规则方法对兼词、歧义识别比较擅长，但是规则总是覆盖不全。因此两者结合再好不过，先通过规则排歧，再通过统计标注，最后经过校对，可以得到正确的标注结果。在两者结合的词性标注方法中，有一种思路可以充分发挥两者优势，避免劣势，就是首选统计方法标注，同时计算计算它的置信度或错误率，这样来判断是否结果是否可疑，在可疑情况下采用规则方法来进行歧义消解，这样达到最佳效果。

### 校验
**第一种校验方法就是检查词性标注的一致性。**一致性指的是在所有标注的结果中，具有相同语境下同一个词的标注是否都相同，那么是什么原因导致的这种不一致呢？一种情况就是这类词就是兼类词，可能被标记为不同词性。另一种情况是非兼类词，但是由于人工校验或者其他原因导致标记为不同词性。达到100%的一致性是不可能的，所以我们需要保证一致性处于某个范围内，由于词数目较多，词性较多，一致性指标无法通过某一种计算公式来求得，因此可以基于聚类和分类的方法，根据欧式距离来定义一致性指标，并设定一个阈值，保证一致性在阈值范围内。

**第二种校验方法就是词性标注的自动校对。**自动校对顾名思义就是不需要人参与，直接找出错误的标注并修正，这种方法更适用于一个词的词性标注通篇全错的情况，因为这种情况基于数据挖掘和规则学习方法来做判断会相对比较准确。通过大规模训练语料来生成词性校对决策表，然后根据这个决策表来找通篇全错的词性标注并做自动修正。

## 句法分析
句法分析分为句法结构分析和依存关系分析。句法结构分析也就是短语结构分析比如提取出句子中的名次短语、动词短语等，最关键的是人可以通过经验来判断的短语结构

### 句法分析树
样子如下：
         -吃(v)-

|                      |

我(rr)            肉(n)

#### 句法结构分析基本方法
目前最成功的是基于概率上下文无关文法(PCFG)。基于PCFG分析需要有如下几个要素：终结符集合、非终结符集合、规则集。
例子是这样的：我们的终结符集合是：∑={我, 吃, 肉,……}，这个集合表示这三个字可以作为句法分析树的叶子节点，当然这个集合里还有很多很多的词

我们的非终结符集合是：N={S, VP, ……}，这个集合表示树的非页子节点，也就是连接多个节点表达某种关系的节点，这个集合里也是有很多元素

```
我们的规则集：R={

NN->我    0.5

Vt->吃     1.0

NN->肉   0.5

VP->Vt NN    1.0

S->NN VP 1.0

……

}
```

这里的句法规则符号可以参考词性标注，后面一列是模型训练出来的概率值，也就是在一个固定句法规则中NN的位置是“我”的概率是0.5，NN推出“肉”的概率是0.5，0.5+0.5=1，也就是左部相同的概率和一定是1。

那么如何根据以上的几个要素来生成句法分析树呢？

（1）“我”

词性是NN，推导概率是0.5，树的路径是“我”

（2）“吃”

词性是Vt，推导概率是1.0，树的路径是“吃”

（3）“肉”

词性是NN，概率是0.5，和Vt组合符合VP规则，推导概率是0.5*1.0*1.0=0.5，树的路径是“吃肉”

NN和VP组合符合S规则，推导概率是0.5*0.5*1.0=0.25，树的路径是“我吃肉”

所以最终的树结构是：

S——|

|        |

NN    VP

我      |——|

          Vt     NN

          吃     肉
          
动态规划算法构建句法结构树
提到动态规划算法，就少不了“选择”的过程，一句话的句法结构树可能有多种，我们只选择概率最大的那一种作为句子的最佳结构，这也是“基于概率”上下文无关文法的名字起源。

上面的计算过程总结起来就是：设W={ω1ω2ω3……}表示一个句子，其中的ω表示一个词(word)，利用动态规划算法计算非终结符A推导出W中子串ωiωi+1ωi+2……ωj的概率，假设概率为αij(A)，那么有如下递归公式：

αij(A)=P(A->ωi)

αij(A)=∑∑P(A->BC)αik(B)α(k+1)j(C)

以上两个式子好好理解一下其实就是上面“我吃肉”的计算过程

### 句法规则提取方法与PCFG的概率参数估计
首先我们需要大量的树库，也就是训练数据。然后我们把树库中的句法规则提取出来生成我们想要的结构形式，并进行合并、归纳等处理，最终得到上面∑、N、R的样子。其中的概率参数计算方法是这样的：

先给定参数为一个随机初始值，然后采用EM迭代算法，不断训练数据，并计算每条规则使用次数作为最大似然计算得到概率的估值，这样不断迭代更新概率，最终得出的概率可以认为是符合最大似然估计的精确值。

## 词义消歧
### 有监督的词义消歧方法
#### 基于互信息的词义消歧方法
这个方法的名字不好理解，但是原理却非常简单：用两种语言对照着看，比如：中文“打人”对应英文“beat a man”，而中文“打酱油”对应英文“buy some sauce”。这样就知道当上下文语境里有“人”的时候“打”的含义是beat，当上下文语境里有“酱油”的时候“打”的含义是buy。按照这种思路，基于大量中英文对照的语料库训练出来的模型就可以用来做词义消歧了，这种方法就叫做基于“互信息”的词义消歧方法。讲到“互信息”还要说一下它的起源，它来源于信息论，表达的是一个随机变量中包含另一个随机变量的信息量(也就是英文信息中包含中文信息的信息量)，假设两个随机变量X、Y的概率分别是p(x), p(y)，它们的联合分布概率是p(x,y)，那么互信息计算公式是：

**I(X; Y) = ∑∑p(x,y)log(p(x,y)/(p(x)p(y)))**
以上公式是怎么推导出来的呢？比较简单，“互信息”可以理解为一个随机变量由于已知另一个随机变量而减少的不确定性(也就是理解中文时由于已知了英文的含义而让中文理解更确定了)，因为“不确定性”就是熵所表达的含义，所以：

**I(X; Y) = H(X) - H(X|Y)**
等式后面经过不断推导就可以得出上面的公式，对具体推导过程感兴趣可以百度一下。

那么我们在对语料不断迭代训练过程中I(X; Y)是不断减小的，算法终止的条件就是I(X; Y)不再减小。

基于互信息的词义消歧方法自然对机器翻译系统的效果是最好的，但它的缺点是：双语语料有限，多种语言能识别出歧义的情况也是有限的(比如中英文同一个词都有歧义就不行了)。

#### 基于贝叶斯分类器的消歧方法
提到贝叶斯那么一定少不了条件概率，这里的条件指的就是上下文语境这个条件，任何多义词的含义都是跟上下文语境相关的。假设语境(context)记作c，语义(semantic)记作s，多义词(word)记作w，那么我要计算的就是多义词w在语境c下具有语义s的概率，即：

p(s|c)
那么根据贝叶斯公式：

p(s|c) = p(c|s)p(s)/p(c)
我要计算的就是p(s|c)中s取某一个语义的最大概率，因为p(c)是既定的，所以只考虑分子的最大值：

s的估计=max(p(c|s)p(s))
因为语境c在自然语言处理中必须通过词来表达，也就是由多个v(词)组成，那么也就是计算：

max(p(s)∏p(v|s))
请尊重原创，转载请注明来源网站www.shareditor.com以及原始链接地址

下面就是训练的过程了：

p(s)表达的是多义词w的某个语义s的概率，可以统计大量语料通过最大似然估计求得：

p(s) = N(s)/N(w)
p(v|s)表达的是多义词w的某个语义s的条件下出现词v的概率，可以统计大量语料通过最大似然估计求得：

p(v|s) = N(v, s)/N(s)
训练出p(s)和p(v|s)之后我们对一个多义词w消歧的过程就是计算(p(c|s)p(s))的最大概率的过程

## 语义角色标注
基于短语结构树的语义角色标注方法、基于浅层句法分析结果的语义角色标注方法、基于依存句法分析结果的语义角色标注方法。但无论哪种方法，过程都是：
句法分析->候选论元剪除->论元识别->论元标注->语义角色标注结果
其中论元剪除就是在较多候选项中去掉肯定不是论元的部分
其中论元识别是一个二值分类问题，即：是论元和不是论元
其中论元标注是一个多值分类问题

## 信息检索方法
### TF-IDF
TF(term frequency)，表示一个词在一个文档中出现的频率；IDF(inverse document frequency)，表示一个词出现在多少个文档中。

它的思路是这样的：同一个词在短文档中出现的次数和在长文档中出现的次数一样多时，对于短文档价值更大；一个出现概率很低的词一旦出现在文档中，其价值应该大于其他普遍出现的词。

## 词向量的应用
1. 同义词。word2vec通过训练好的词向量，指定一个词可以返回和它cos距离最相近的词排序
2. 词性标注和语义角色标注任务。具体使用方法是：把词向量作为神经网络的输入层，通过前馈网络和卷积网络完成。
3. 句法分析和情感分析任务。具体使用方法是：把词向量作为递归神经网络的输入。
4. 命名实体识别和短语识别。具体使用方法是：把词向量作为扩展特征使用。

另外词向量有一个非常特别的现象：C(king)-C(queue)≈C(man)-C(woman)，这里的减法就是向量逐维相减，换个表达方式就是：C(king)-C(man)+C(woman)和它最相近的向量就是C(queue)，这里面的原理其实就是：语义空间中的线性关系。基于这个结论相信会有更多奇妙的功能出现。

## word2vec
CBOW已知当前词上下文前提下预测当前词，Skip-gram模型已知词的情况下预测上下文

## 基于美剧字幕的聊天语料库建设方案

