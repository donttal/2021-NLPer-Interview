# 对话系统理论需要解决问题和方案
[toc]

[专栏链接](https://zhuanlan.zhihu.com/p/33219577)
# 1，系统倾向于产生“I don't know”这种universal、dull response
## **深度学习对话系统理论篇--MMI模型**
论文来源：李纪为的论文“A Diversity-Promoting Objective Function for Neural Conversation Models”
### 概要
违章提出使用MMI代替原始的maximum likelihood作为目标函数，目的是使用互信息减小“I don't Know”这类无聊响应的生成概率。一般的seq2seq模型，倾向于生成安全、普适的响应，因为这种响应更符合语法规则，在训练集中出现频率也较高，最终生成的概率也最大，而有意义的响应生成概率往往比他们小。通过MMI来计算输入输出之间的依赖性和相关性，可以减少模型对他们的生成概率。本文提出了两种模型（其实就是改了下目标函数，而且训练过程中仍然使用likelihood，仅在测试的时候使用新的目标函数将有意义的响应的概率变大~~），MMI-antiLM和MMI-bidi，下面分别进行介绍。

### 新的目标函数
在介绍模型之前先来看看新的目标函数和普通的目标函数的区别，以便清楚地明白新目标函数的作用和功能。首先看下原始的目标函数，就是在给定输入S的情况下生成T的概率，其实就是一个T中每个单词出现的条件概率的连乘
$$\hat{T}=\underset{T}{\arg \max }\{\log p(T | S)\}$$

接下来看提出的第一个目标函数MMI-antiLM，在其基础上添加了目标序列本身的概率logp(T)，p(T)就是一句话存在的概率，也就是一个模型，前面的lambda是惩罚因子，越大说明对语言模型惩罚力度越大。由于这里用的是减号，所以相当于在原本的目标上减去语言模型的概率，也就降低了“I don't know”这类高频句子的出现概率。
![](https://pic2.zhimg.com/80/v2-a73997ac502e0be7babe46faa7129c9d_hd.jpg)

然后是第二个目标函数MMI-bidi，在原始目标函数基础上添加logp(S|T)，也就是T的基础上产生S的概率，而且可以通过改变lambda的大小衡量二者的重要性。后者可以表示在响应输入模型时产生输入的概率，自然像“I don't know”这种答案的概率会比较低，而这里使用的是相加，所以会降低这种相应的概率。接下来我们再详细介绍两个模型的细节
![](https://pic2.zhimg.com/80/v2-642c203bab7316046816d91115e5b5ed_hd.jpg)

### MMI-antiLM
如上所说，MMI-antiLM模型使用第一个目标函数，引入了logp(T)，如果lambda取值不合适可能会导致产生的响应不符合语言模型，所以在实际使用过程中会对其进行修正。由于解码过程中往往第一个单词或者前面几个单词是根据encode向量选择的，后面的单词更倾向于根据前面decode的单词和语言模型选择，而encode的信息影响较小。也就是说我们只需要对前面几个单词进行惩罚，后面的单词直接根据语言模型选择即可，这样就不会使整个句子不符合语言模型了。使用下式中的U(T)代替p(T),式中g(k)表示要惩罚的句子长度：
![](https://pic2.zhimg.com/80/v2-a210c64b458d082e53e51ace1d451579_hd.jpg)

![](https://pic3.zhimg.com/80/v2-62d088ac0d91001e801ddcc40f7d3c16_hd.jpg)

加入响应句子长度
![](https://pic1.zhimg.com/80/v2-143671d99303690f4115f9797e9fe458_hd.jpg)

### MMI-bidi
MMI-bidi模型引入了p(S|T)项，这就需要先计算出完整的T序列再将其传入一个提前训练好的反向seq2seq模型中计算该项的值。但是考虑到S序列会产生无数个可能的T序列，我们不可能将每一个T都进行计算，所以这里引入beam-search只计算前200个序列T来代替。然后再计算两项和，进行得分重排。论文中也提到了这么做的缺点，比如最终的效果会依赖于选择的前N个序列的效果等等，但是实际的效果还是可以的。

### 实验结果
最终在Twitter和OpenSubtitle两个数据集上面进行测试，效果展示BLEU得分逗比标准的seq2seq模型要好。
![](https://pic1.zhimg.com/80/v2-691e12e1e9b4b1bb325ac81c0089bcb0_hd.jpg)


## 深度学习对话系统理论篇--seq2BF

# 2，chatbot对话的前后一致性问题
深度学习对话系统理论篇--Persona based Neural Model

# 3，长期多轮对话问题
## 使用Memory Networks实现对话状态追踪（参考我的专栏Memory Networks）

### 神经网络的端到端对话系统优缺点
不需要像传统对话系统的状态跟踪组件，直接根据对话的上下文和用户当前输入生成回复，并且可实现端到端的反向传播训练。但是这就导致了其没有办法使用之前的数据集进行训练和测试（都针对状态跟踪设计）。所以目前一般使用人工评分（很难扩展）或者BLEU得分来评价模型的效果，但是往往不能够达到评价的标准。

### 论文evaluating prerequisite qualities for learning end-to-end dialog system阅读笔记
主要贡献：提出一个评估端到端对话系统性能的指标以及相关的数据集。

作者认为一个对话系统应该具有下面的四种能力才可以很方便的推广到其他领域中。作者以电影推荐助手为切入点，使用OMDb、MovieLens、Reddit构建了四个数据集，分别解决四个问题

- QA：用于测试对话系统能否回答客观问题，类似于一个给予KB知识库的问答系统，给予SimpleQuestions数据集进行修改以适应本文的要求。其中每个问题会有一个答案列表，模型只需要生成一个答案列表即可，而不需要生成自然语言对话。最终使用@1作为评价指标。
- 推荐：用于测试系统对用户个性化推荐的能力，而不是像上面的通用回答一样。基于MovieLens数据集，构建user*item矩阵记录用户给电影打分，然后在生成仿真对话，对每个用户选择其打5分的电影，然后推荐一个相似的电影。其实就是讲用户喜欢的电影作为上下文，然后需要给出用户潜在喜欢的电影，如上图所示，与QA任务相同，本数据集也是生成一个答案列表即可，不过区别在于这里以@100作为评估指标而不是@1，因为推荐矩阵十分稀疏@1准确率会很低。
- QA+推荐混合能力：上面两个任务都只涉及一轮对话，也就是一个问题对应一个回答，这个任务主要关注于多轮对话。每条数据会有3轮对话，第一轮是Recommendation，第二轮是QA，第三轮也是类似于Recommendation的相似电影推荐任务。每一轮的对话都需要对之前对话的理解和上下文信息的使用。而且会对三轮对话的回答都进行评估，，并且使用@10作为评价指标。
- chit-chat，闲聊能力：这个数据集是为了评测对话系统的chit-chat闲聊能力。使用Reddit的电影评价回复（两个用户之间）数据构建，76%是单轮回复，17%是两轮，7%是三轮及以上。为了跟前面的几个任务匹配，这里讲对话生成转化为目标选择。即选择10000个没有在数据集中出现过的评论作为负样本，每次的目标是从10001（10000个负样本加1个正确答案）候选答案中选择正确答案。
- Joint task：将上面的4个任务进行联合训练，这样我们的模型将具有chit-chat（任务4）能力和目的性回答（任务1-3）。

最终作者分别使用了**Memory Networks**、Supervised Embedding models、LSTMs、QA、SVD、IR等方法对上述五个任务进行了测试，发现MemNN和Supervised Embedding效果比较好，而相比之下，MemNN效果是最好的。结果如下图所示：
![](https://pic1.zhimg.com/80/v2-aca2d62826213716a53eaafd6587a614_hd.jpg

本文还提出了一个比较好的想法就是将记忆分为Long-Term Memory和Short-Term Memory，其中Long指的是KB等知识库的记忆，用于回答一些客观的问题，使用三元组存储，而Short指的是当前对话的对话历史，更切合每条数据本身的记忆，可以得到一些对用户主观的了解和回答。这样综合使用Long和Short可以有比较好的回答效果，而且可以很方便地将4个任务joint起来进行训练。)
![](https://pic3.zhimg.com/80/v2-6e77324dcd14f0767f41a4f02ddea466_hd.jpg)

### Dialog-based Language Learning
主要贡献在于其提出了10个基于对话的监督任务数据集
![](https://pic4.zhimg.com/80/v2-c30d41ef2aba2a3001bad19a1ba5f653_hd.jpg)
1. **Imitating an Expert Student：**模仿专家进行对话，需要直接回答正确答案。可以看做剩余任务的baseline。
2. **Positive and Negative Feedback**：带有反馈的对话，回答问题会收到对或者错的提示，而且对于正确的回答会有额外的reward信号作为奖励（数据集中使用+表示）。
3. **Answers Supplied by Teacher**：当回答错误时，会被告知正确答案。难度介于任务1和2之间；
4. **Hints Supplied by Teacher**：回答错误时，会被给关于正确答案的提示，难度介于任务2和3之间；
5. **Supporting Facts Supplied by Teacher**：回答错误时，会被给关于正确答案的support facts；
6. **Missing Feedback**：有一部分正确答案的reward会丢失（50%），与任务2相对应；
7. **No Feedback**：完全没有reward；
8. **Imitation and Feedback Mixture**：任务1和2的结合，检验模型是否可以同时学习两种监督任务；
9. **Asking for Corrections**：当回答错误的时候，会主动请求帮助“Can you help me”，对应任务3；
10. **Asking for Supporting facts**：当回答错误的时候，会主动请求帮助“Can you help me”，对应任务5；

这10个监督数据及是基于bAbI和movieQA两个数据集构建的，在此之上，作者又提出了下面四种使用数据进行训练的策略：

- Imitation Learning
- Reward-based Imitation
- Forward Prediction
- Reward-based Imitation + Forward Prediction

论文仍然使用MemNN作为模型进行训练，如下图所示：
![](https://pic1.zhimg.com/80/v2-a151ca59e5e595fa03d1cc2f1142e2a4_hd.jpg)



## 深度学习对话系统理论篇--HRED+VHRED+Attention With Intention
### Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models(HRED)论文阅读笔记
![](https://pic3.zhimg.com/80/v2-1bdec724b3bbe58b1be8d30d5df5117a_hd.jpg)
简单来说就是使用分层的seq2seq模型构造多轮对话，Encoder RNN主要用于对输入句子进行编码，这里跟普通的seq2seq没有什么区别，并且把最后一个时刻的隐层向量认为是输入句子的编码向量，当做下一层RNN的输入向量。中间一层context RNN用来编码整个对话的状态、意图等对话层面的信息，而第一层RNN用来编码一句话的句子层面信息，中间层每个时刻输入的第一层输出的句子表示向量，这样context RNN的隐藏层向量就可以记住之前的对话信息，所以成为上下文向量。最后，将该编码了之前对话信息的向量作为decoder RNN的输入向量，使得在解码过程中除了使用回答句子本身信息还会结合对话上下文信息。这就是整体的架构，还有几个小的点需要注意一下：
- Encoder和Decoder RNN每个句子是一个RNN，而Context RN只有一个RNN；
- 在编码和解码阶段，处理每个句子的RNN结构完全相同而且共享参数（“The same encoder RNN and decoder RNN parameters are used for every utterance in a dialogue”），目的是为了生成一般化的语言模型；
- 在解码阶段，每个时刻都会把Context RNN的输出与该时刻的输入并联一块作为输入，目的是是的每个解码时刻都可以引入上下文信息。
但是论文中也有提到这种方法效果并不是很好，反而会受预训练模型的影响比较严重，比如与训练的词向量和适用别的任务对参数预训练，这些预训练的参数对模型的准确度提升有着显著效果。

### VHEAD
![](https://pic3.zhimg.com/80/v2-899ebcc5788b67eea24dc9443b851fb6_hd.jpg)
在传统的seq2seq模型中唯一变化的因素就是输出序列概率分布的不同导致输出发生变化，这在多轮对话中显然是无法满足对话响应的多样性需求。因为一旦输出的第一个词确定之后，后面的单词基本上都是根据语言模型确定，而且每个编码时刻的隐层向量hm更倾向于记住短期依赖而不是全局信息。因此为了引入更多的变化因素，作者引入变分编码的思想，在Context RNN环节加入一个高斯随机变量zn，以增加响应的多样性。

Zn的均值和方差都是根据Context RNN的隐藏层向量计算的，现将其传入一个两层神经网络增加一定的变化性，然后将该神经网络的输出与一个矩阵相乘得到均值，再将该输出与另一个矩阵相乘经过一个softplus函数得到方差。有了均值和方差之后便可以经过采样得到高斯变量Zn，将其与Context RNN拼接就相当于加入了噪声，可以一定程度上增加模型的变化性。

### Attention with Intention
![](https://pic3.zhimg.com/80/v2-26e34c54a8f55e761be66eb04890a7f2_hd.jpg)
其实整体流程与HRED相似，这里就只介绍几个需要注意的细节就行了：

- 每个句子的Encoder RNN的初始化状态都是前面一个Decoder RNN的最后一个隐层状态，这样做是为了在encoder阶段把前面时刻的对话状态引入进去；
- Encoder是输出可以使用最后一个时刻的隐层状态，也可以使用Attention机制对每个时刻隐层状态进行加权求和，然后作为Intention RNN的输入；
- Intention RNN在每个时刻除了会考虑Encoder的输出还会结合前一时刻Decoder的输出；
- Decoder RNN的初始化状态是该时刻Intention RNN的输出

## 深度学习对话系统理论篇--Multiresolution Recurrent Neural Networks

# 4，与DRL强化学习相结合
## 深度学习对话系统理论篇--Deep Reinforcement Learning for Dialogue Generation
来源于论文Deep Reinforcement Learning for Dialogue Generation阅读笔记
### 文章亮点
首先使用Seq-to-Seq模型预训练一个基础模型，然后根据作者提出的三种Reward来计算每次生成的对话的好坏，并使用policy network的方法提升对话响应的多样性、连贯性和对话轮次。文章最大的亮点就在于定义了三种reward（Ease of answering、Information Flow、Semantic Coherence），分别用于解决dull response、repetitive response、ungrammatical response。
![](https://pic1.zhimg.com/80/v2-8767d63cb2cc47245fdee6576334cc60_hd.jpg)
模型采用两个agent之间相互对话的方法进行对话仿真。于是上下文可以看做是p1,q1,p2,q2...,pn,qn，模型需要根据上下文，通过Seq-to-Seq网络生成一个response，也就是强化学习中的action。由于对话生成，每次的答案都可能不一样，所以可以**将action空间看作是无限的。而状态空间则是前一轮的对话的历史[pi, qi]**。而**策略policy就是Seq-to-Seq模型生成的相应的概率分布**。我们可以把这个问题看成是上下文的对话历史输入到神经网络中，然后输出是一个response的概率分布：pRL(pi+1|pi,qi).所谓策略就是进行随机采样，选择要进行的回答。最后使用policy gradient进行网络参数的训练。
![](https://pic4.zhimg.com/80/v2-e2c4737d6e921dc52e08029ee506ed1f_hd.jpg)

r1是为了提高模型的前瞻性，避免模型产生无聊的dull response而定义的。S是一个预先定义好的dull response的集合，比如“I don't know what you are talking about”等。然后计算当模型产生的响应a作为输入时模型输出s的概率，在对S集合中的每一句话进行求和。因为$P_{seq2seq}$肯定小于1，所以log项小于零，则r1大于零，而且a产生无聊响应s的概率越小，r1越大。通过r1的奖励机制，模型最终产生的action会慢慢的远离dull response，而且也会一定程度上估计到下一个人的回复，让对方可以更容易回复。

![](https://pic3.zhimg.com/80/v2-31dc955c53c6245582fa1757df06bdc6_hd.jpg)

r2是为了增加信息流的丰富程度，避免两次回复之间相似程度很高的情况。所以r2使用余弦相似度来计算两个句子之间的相似程度，很容易发现r2也是一个大于零的数，用来惩罚相似的句子，两个句子越相似，余弦值越接近于1，r2越小，反之奖励越大。

r3是为了语义连贯性，避免模型只产生那些高reward的响应，而丧失回答的充分性和连贯性。为了解决这个问题模型采用互信息来实现，由两部分组成，分别是基于pi，qi上下文产生a的概率和基于a产生qi的概率。反向的seq2seq是使用source和target反过来训练的另外一个模型，这样做的目的是为了提高q和a之间的相互关系，让对话更具有可持续性。可以看出来，r3的两项都是负值。
![](https://pic3.zhimg.com/80/v2-bf57bfccab02dbe9dd20658d21283f92_hd.jpg)

![](https://pic1.zhimg.com/80/v2-c8f2e9b7fcbc61242e46a8b7ca5917bc_hd.jpg)

对r1，r2，r3进行加权求和，权值也是很简单的直接分配，这样最终r应该会在0附近，时正时负的奖励值。最后总模型在训练的时候也是先使用Seq-to-Seq模型先预训练一个基础模型，然后在其基础上在使用reward进行policy gradient的训练来优化模型的效果

### 实验结果
- 对话的长度，作者认为当对话出现dull response的时候就算做对话结束，所以使用对话的轮次来作为了评价指标：
![](https://pic4.zhimg.com/80/v2-469e8a71a68d8a63dc94494106e1c81f_hd.jpg)
- 不同unigrams、bigrams元组的数量和多样性，用于评测模型产生回答的丰富程度：
![](https://pic1.zhimg.com/80/v2-5f77914480d2c646fe4fd536925a11b0_hd.jpg)
- 人类评分
![](https://pic4.zhimg.com/80/v2-b946e210edd9495ea7d8a562fe43718f_hd.jpg)
最终对话效果如下图所示：
![](https://pic1.zhimg.com/80/v2-d4963f1cad052424dffee97cddd9ce18_hd.jpg)

## 深度学习对话系统理论篇--Policy Networks with Two-Stage Training for Dialogue Systems
## 深度学习对话系统理论篇--Deep Reinforcement Learning for Goal-Oriented Dialogues

# 5，与GAN对抗生成网络结合