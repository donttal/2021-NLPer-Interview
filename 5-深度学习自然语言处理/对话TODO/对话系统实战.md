# 对话系统实战
## ParlAI学习
### 基本使用
```python
# display examples from bAbI 10k task 1
python examples/display_data.py -t babi:task10k:1

# train MemNN using batch size 1 and 4 threads for 5 epochs
python examples/train_model.py -t babi:task10k:1 -mf /tmp/babi_memnn -bs 1 -nt 4 -eps 5 -m memnn --no-cuda

# display predictions for model save at specified file on bAbI task 1
python examples/display_model.py -t babi:task10k:1 -mf /tmp/babi_memnn -ecands vocab

# interact with saved model
python examples/interactive.py -mf /tmp/babi_memnn -ecands vocab
...
Enter your message: John went to the hallway.\n Where is John?
```

### Train a Transformer on Twitter（实际项目）
- Let’s begin again by printing the first few examples.
```
# display first examples from twitter dataset
python examples/display_data.py -t twitter
```
- we’ll train the model. This will take a while to reach convergence.
```
# train transformer ranker
python examples/train_model.py -t twitter -mf /tmp/tr_twitter -m transformer/ranker -bs 10 -vtim 3600 -cands batch -ecands batch --data-parallel True
```
-bs 批处理大小
-vtim 每3600秒运行一次验证
该训练模型脚本会在训练结束时根据有效集和测试集对模型进行评估，但是如果我们要评估保存的模型-也许是将新训练过的Transformer的结果与预先训练的convai2（seq2seq基线）进行比较， 我们可以执行以下操作
```
# Evaluate seq2seq model trained on convai2 from our model zoo
python examples/eval_model.py -t twitter -m legacy:seq2seq:0 -mf models:convai2/seq2seq/convai2_self_seq2seq_model
```
Finally, let’s print some of our transformer’s predictions with the same display_model script from above
```
# display predictions for model saved at specific file on twitter
python examples/display_model.py -t twitter -mf /tmp/tr_twitter -ecands batch
```

## stanford chat项目
[项目地址](https://github.com/chiphuyen/stanford-tensorflow-tutorials/tree/master/assignments/chatbot)

## ChatterBot

## DeepQA项目

