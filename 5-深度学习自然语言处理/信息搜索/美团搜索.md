# BERT改进美团搜索
[网址](https://tech.meituan.com/2020/07/09/bert-in-meituan-search.html)
## 字面相关性
早期的相关性匹配主要是根据Term的字面匹配度来计算相关性，但字面匹配有它的局限，主要表现在：

* 词义局限：字面匹配无法处理同义词和多义词问题，如在美团业务场景下“宾馆”和“旅店”虽然字面上不匹配，但都是搜索“住宿服务”的同义词；而“COCO”是多义词，在不同业务场景下表示的语义不同，可能是奶茶店，也可能是理发店。

* 结构局限：“蛋糕奶油”和“奶油蛋糕”虽词汇完全重合，但表达的语义完全不同。 当用户搜“蛋糕奶油”时，其意图往往是找“奶油”，而搜“奶油蛋糕”的需求基本上都是“蛋糕”。

## BERT语义相关性
* Feature-based：属于基于表示的语义匹配方法。类似于DSSM双塔结构，通过BERT将Query和Doc编码为向量，Doc向量离线计算完成进入索引，Query向量线上实时计算，通过近似最近邻（ANN）等方法实现相关Doc召回。

* Finetune-based：属于基于交互的语义匹配方法，将Query和Doc对输入BERT进行句间关系Fine-tuning，最后通过MLP网络得到相关性分数。 

## 数据增强
如果采用人工标注的方法为每个业务领域标注一批训练样本，时间和人力成本过高。我们的解决办法是使用美团搜索积累的大量用户行为数据（如浏览、点击、下单等）， 这些行为数据可以作为弱监督训练数据。在DSSM模型进行样本构造时，每个Query下抽取1个正样本和4个负样本，这是比较常用的方法，但是其假设Query下的Doc被点击就算是相关的，这个假设在实际的业务场景下会给模型引入一些噪声。

## 模型优化
### 知识融合
美团搜索场景中的Query和Doc都以短文本为主，我们尝试在预训练和Fine-tuning阶段融入图谱品类和实体信息，弥补Query和Doc文本信息的不足，强化语义匹配效果。

在相关性判别任务中，BERT模型的输入是对。对于每一个输入的Token，它的表征由其对应的词向量（Token Embedding）、片段向量（Segment Embedding）和位置向量（Position Embedding）相加产生。为了引入Doc品类信息，我们将Doc三级品类信息拼接到Doc标题之后，然后跟Query进行相关性判断，如图4所示。

![](https://p0.meituan.net/travelcube/af2a6102e46958b4301770dcb47a560643892.png)

### 训练任务修改 
我们对MT-BERT的预训练方式做了相应改进，BERT预训练的目标之一是NSP（Next Sentence Prediction），在搜索场景中没有上下句的概念，在给定用户的搜索关键词和商户文本信息后，判断用户是否点击来取代NSP任务。

## 模型部署
TF-Serving在线模型服务：L2排序模型、BERT模型上线使用TF-Serving进行部署。TF-Serving预测引擎支持Faster Transformer[38]加速BERT推理，提升了线上的预估速度。

为了进一步提升性能，我们将头部Query进行缓存只对长尾Query进行在线打分，线上预估结合缓存的方式，即节约了GPU资源又提升了线上预估速度。经过上述优化，我们实现了50 QPS下，L2模型TP99只升高了2ms，满足了上线的要求。