# 神经网络调参
[toc]
## 神经网络的超参数分类

神经网路中的超参数主要包括**1. 学习率 η，2. 正则化参数 λ，3. 神经网络的层数 L，4. 每一个隐层中神经元的个数 j，5. 学习的回合数Epoch，6. 小批量数据 minibatch 的大小，7. 输出神经元的编码方式，8. 代价函数的选择，9. 权重初始化的方法，10. 神经元激活函数的种类，11.参加训练模型数据的规模 这十一类超参数。**

这些都是可以影响神经网络学习速度和最后分类结果，其中神经网络的学习速度主要根据训练集上代价函数下降的快慢有关，而最后的分类的结果主要跟在验证集上的分类正确率有关。因此可以根据该参数主要影响代价函数还是影响分类正确率进行分类。

1. 超参数正则化参数 λ，神经网络的层数 L，每一个隐层中神经元的个数 j， 输出神经元的编码方式主要影响的时神经网络的分类正确率
2. 权重初始化的方法 主要影响代价函数曲线下降速度，同时有时也会影响正确率
3. 学习率 η，代价函数的选择，神经元激活函数的种类主要影响学习速度，这点主要体现在训练数据代价函数曲线的下降速度上
4. 学习的回合数Epoch，小批量数据 minibatch 的大小，参加训练模型数据的规模主要影响模型分类正确率和训练用总体时间。

## 编程安排

由于深度学习实验超参众多，代码风格良好的实验环境，可以让你的人工或者自动调参更加省力，有以下几点可能需要注意：

- 将各个参数的设置部分集中在一起。如果参数的设置分布在代码的各个地方，那么修改的过程想必会非常痛苦。
- 可以输出模型的损失函数值以及训练集和验证集上的准确率。
- 可以考虑设计一个子程序，可以根据给定的参数，启动训练并监控和周期性保存评估结果。再由一个主程序，分配参数以及并行启动一系列子程序。

## 画图

训练数据遍历一轮以后，就输出一下训练集和验证集准确率。同时画到一张图上。这样训练一段时间以后，如果模型一直没有收敛，那么就可以停止训练，尝试其他参数了，以节省时间。
如果训练到最后，训练集，测试集准确率都很低，那么说明模型有可能欠拟合。那么后续调节参数方向，就是增强模型的拟合能力。例如增加网络层数，增加节点数，减少dropout值，减少L2正则值等等。
如果训练集准确率较高，测试集准确率比较低，那么模型有可能过拟合，这个时候就需要向提高模型泛化能力的方向，调节参数。

## 从粗到细分阶段调参

实践中，一般先进行初步范围搜索，然后根据好结果出现的地方，再缩小范围进行更精细的搜索。

1. 建议先参考相关论文，以论文中给出的参数作为初始参数。至少论文中的参数，是个不差的结果。
2. 如果找不到参考，那么只能自己尝试了。可以先从比较重要，对实验结果影响比较大的参数开始，同时固定其他参数，得到一个差不多的结果以后，在这个结果的基础上，再调其他参数。例如学习率一般就比正则值，dropout值重要的话，学习率设置的不合适，不仅结果可能变差，模型甚至会无法收敛。
3. 如果实在找不到一组参数，可以让模型收敛。那么就需要检查，是不是其他地方出了问题，例如模型实现，数据等等。可以参考我写的[深度学习网络调试技巧](https://zhuanlan.zhihu.com/p/20792837)

### 调整参数步骤

在训练神经网络的时候，需要调的参数很多，选一组最适合自己模型的参数。实际训练的时候如果不按照一定的顺序，会很乱。因此正确有序地调参很重要，需要调节的参数大概有如下几个[1]：

神经网络的层数
每层神经元的个数
如何初始化Weights和biases
loss函数选择哪一个
选择何种Regularization？L1,L2
Regularization parameter lambda 选择多大合适
激励函数如何选择
是否使用dropout
训练集多大比较合适
mini-batch选择多大
学习率多少合适
选择何种梯度下降算法
何时停止Epoch训练
自己模型的超参数

## 提高速度

调参只是为了寻找合适的参数，而不是产出最终模型。一般在小数据集上合适的参数，在大数据集上效果也不会太差。因此可以尝试对数据进行精简，以提高速度，在有限的时间内可以尝试更多参数。

- 对训练数据进行采样。例如原来100W条数据，先采样成1W，进行实验看看。
- 减少训练类别。例如手写数字识别任务，原来是10个类别，那么我们可以先在2个类别上训练，看看结果如何。

**从简单的开始实验：MNIST数据集，可以先简化使用0,1两类图，减少百分之八十的数据量，用两层神经网络[784,10](比[784,32,10])要快上不少。**

***更快地取得反馈：之前每个epoch来检测准确率，可以替换为每一千幅图片后，或者减少validation set的量，比如用100代替10,000\***

深度学习中经常看到epoch、 iteration和batchsize

- batchsize：批大小。在深度学习中，一般采用SGD训练，即每次训练在训练集中取batchsize个样本训练；
- iteration：1个iteration等于使用batchsize个样本训练一次；
- epoch：1个epoch等于使用训练集中的全部样本训练一次；

举个例子，训练集有1000个样本，batchsize=10，训练完整个样本集需要：100次iteration，1次epoch

## 超参数范围

建议优先在对数尺度上进行超参数搜索。比较典型的是学习率和正则化项，我们可以从诸如0.001 0.01 0.1 1 10，以10为阶数进行尝试。因为他们对训练的影响是相乘的效果。不过有些参数，还是建议在原始尺度上进行搜索，例如dropout值: 0.3 0.5 0.7)。

## 经验参数

这里给出一些参数的经验值，避免大家调参的时候，毫无头绪。

- learning rate: 1 0.1 0.01 0.001, 一般从1开始尝试。很少见learning rate大于10的。学习率一般要随着训练进行衰减。衰减系数一般是0.5。 衰减时机，可以是验证集准确率不再上升时，或固定训练多少个周期以后。
  不过更建议使用自适应梯度的办法，例如adam,adadelta,rmsprop等，这些一般使用相关论文提供的默认值即可，可以避免再费劲调节学习率。对RNN来说，有个经验，如果RNN要处理的序列比较长，或者RNN层数比较多，那么learning rate一般小一些比较好，否则有可能出现结果不收敛，甚至Nan等问题。
- 网络层数： 先从1层开始。
- 每层结点数： 16 32 128，超过1000的情况比较少见。超过1W的从来没有见过。
- batch size: 128上下开始。batch size值增加，的确能提高训练速度。但是有可能收敛结果变差。如果显存大小允许，可以考虑从一个比较大的值开始尝试。因为batch size太大，一般不会对结果有太大的影响，而batch size太小的话，结果有可能很差。
- clip c(梯度裁剪): 限制最大梯度,其实是value = sqrt(w1^2+w2^2….),如果value超过了阈值，就算一个衰减系系数,让value的值等于阈值: 5,10,15
- dropout： 0.5
- L2正则：1.0，超过10的很少见。
- 词向量embedding大小：128，256
- 正负样本比例： 这个是非常忽视，但是在很多分类问题上，又非常重要的参数。很多人往往习惯使用训练数据中默认的正负类别比例，当训练数据非常不平衡的时候，模型很有可能会偏向数目较大的类别，从而影响最终训练结果。除了尝试训练数据默认的正负类别比例之外，建议对数目较小的样本做过采样，例如进行复制。提高他们的比例，看看效果如何，这个对多分类问题同样适用。
  在使用mini-batch方法进行训练的时候，尽量让一个batch内，各类别的比例平衡，这个在图像识别等多分类任务上非常重要。



## 自动调参

人工一直盯着实验，毕竟太累。自动调参当前也有不少研究。下面介绍几种比较实用的办法：

- Gird Search. 这个是最常见的。具体说，就是每种参数确定好几个要尝试的值，然后像一个网格一样，把所有参数值的组合遍历一下。优点是实现简单暴力，如果能全部遍历的话，结果比较可靠。缺点是太费时间了，特别像神经网络，一般尝试不了太多的参数组合。

- Random Search。Bengio在[Random Search for Hyper-Parameter Optimization](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)中指出，Random Search比Gird Search更有效。实际操作的时候，一般也是先用Gird Search的方法，得到所有候选参数，然后每次从中随机选择进行训练。

- Bayesian Optimization. 贝叶斯优化，考虑到了不同参数对应的实验结果值，因此更节省时间。和网络搜索相比简直就是老牛和跑车的区别。具体原理可以参考这个论文：

   

  Practical Bayesian Optimization of Machine Learning Algorithms

   

  ，这里同时推荐两个实现了贝叶斯调参的Python库，可以上手即用：

  - [jaberg/hyperopt](https://github.com/jaberg/hyperopt), 比较简单。
  - [fmfn/BayesianOptimization](https://github.com/fmfn/BayesianOptimization)， 比较复杂，支持并行调参。

## Nan解决方案

Nan问题，我相信大部分人都遇到过，一般可能是下面几个原因造成的：

1. 除0问题。这里实际上有两种可能，一种是被除数的值是无穷大，即Nan，另一种就是除数的值是0。之前产生的Nan或者0，有可能会被传递下去，造成后面都是Nan。请先检查一下神经网络中有可能会有除法的地方，例如softmax层，再认真的检查一下数据。我有一次帮别人调试代码，甚至还遇到过，训练数据文件中，有些值就是Nan。。。这样读进来以后，开始训练，只要遇到Nan的数据，后面也就Nan了。可以尝试加一些日志，把神经网络的中间结果输出出来，看看哪一步开始出现Nan。后面会介绍Theano的处理办法。


2. 梯度过大，造成更新后的值为Nan。特别是RNN，在序列比较长的时候，很容易出现梯度爆炸的问题。一般有以下几个解决办法。
   1. 对梯度做clip(梯度裁剪），限制最大梯度,其实是value = sqrt(w1^2+w2^2….),如果value超过了阈值,就算一个衰减系系数,让value的值等于阈值: 5,10,15。
   2. 减少学习率。初始学习率过大，也有可能造成这个问题。需要注意的是，即使使用adam之类的自适应学习率算法进行训练，也有可能遇到学习率过大问题，而这类算法，一般也有一个学习率的超参，可以把这个参数改的小一些。
 
3. 初始参数值过大，也有可能出现Nan问题。输入和输出的值，最好也做一下归一化。

## 神经网络学不到东西解决思路

可能我们并没有遇到，或者解决了Nan等问题，网络一直在正常的训练，但是cost降不下来，预测的时候，结果不正常。

1. 请打印出训练集的cost值和测试集上cost值的变化趋势，正常情况应该是训练集的cost值不断下降，最后趋于平缓，或者小范围震荡，测试集的cost值先下降，然后开始震荡或者慢慢上升。如果训练集cost值不下降，有可能是代码有bug，有可能是数据有问题（本身有问题，数据处理有问题等等），有可能是超参（网络大小，层数，学习率等）设置的不合理。
   请人工构造10条数据，用神经网络反复训练，看看cost是否下降，如果还不下降，那么可能网络的代码有bug，需要认真检查了。如果cost值下降，在这10条数据上做预测，看看结果是不是符合预期。那么很大可能网络本身是正常的。那么可以试着检查一下超参和数据是不是有问题。
2. 如果神经网络代码，全部是自己实现的，那么强烈建议做梯度检查。确保梯度计算没有错误。
3. 先从最简单的网络开始实验，不要仅仅看cost值，还要看一看神经网络的预测输出是什么样子，确保能跑出预期结果。例如做语言模型实验的时候，先用一层RNN，如果一层RNN正常，再尝试LSTM，再进一步尝试多层LSTM。
4. 如果可能的话，可以输入一条指定数据，然后自己计算出每一步正确的输出结果，再检查一下神经网络每一步的结果，是不是一样的。

## 参数初始化

下面几种方式,随便选一个,结果基本都差不多。但是一定要做。否则可能会减慢收敛速度，影响收敛结果，甚至造成Nan等一系列问题。
n_in为网络的输入大小，n_out为网络的输出大小，n为n_in或(n_in+n_out)*0.5
Xavier初始法论文：http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf
He初始化论文：https://arxiv.org/abs/1502.01852

- uniform均匀分布初始化：
  w = np.random.uniform(low=-scale, high=scale, size=[n_in,n_out])
  - Xavier初始法，适用于普通激活函数(tanh,sigmoid)：scale = np.sqrt(3/n)
  - He初始化，适用于ReLU：scale = np.sqrt(6/n)
- normal高斯分布初始化：
  w = np.random.randn(n_in,n_out) * stdev # stdev为高斯分布的标准差，均值设为0
  - Xavier初始法，适用于普通激活函数 (tanh,sigmoid)：stdev = np.sqrt(n)
  - He初始化，适用于ReLU：stdev = np.sqrt(2/n)
- svd初始化：对RNN有比较好的效果。参考论文：https://arxiv.org/abs/1312.6120

这里提出知乎一篇很好的文章

[论智：可视化超参数作用机制：二、权重初始化zhuanlan.zhihu.com![图标](https://pic4.zhimg.com/v2-69012b8cba1704575aa5d5c818c8f403_180x120.jpg)](https://zhuanlan.zhihu.com/p/38315135)



## 数据预处理方式

**归一化处理**

```text
X /= np.std(X, axis=0)
```

归一化处理是指每个维度都通过标准偏差来进行缩放或者确保每个维度最大值和最小值在-1到1之间等等，红线表示数据的范围，中间长度不等，右边长度相等。在图像处理中，归一化并不常用。

**零中心化处理**

```text
x -= np.mean(X, axis=0)
```

零中心化处理是指减去每个维度的平均值，进而使得数据是以零为中心的。数据集中在原点的周围。在图像处理中，零中心化处理是一种常用的数据预处理的方式。

**PCA and Whitening(PCA算法和白化处理)**

PCA和白化是数据预处理的另一种方式。在这个过程中，应用PCA算法将协方差矩阵变成对角矩阵，或者对数据进行白化处理，那意味着在PCA处理后对数据进行压缩，使协方差矩阵变成单位矩阵，这种预处理方式在机器学习中经常用到。



## 训练技巧

- 要做梯度归一化,即算出来的梯度除以minibatch size
- clip c(梯度裁剪): 限制最大梯度,其实是value = sqrt(w1^2+w2^2….),如果value超过了阈值,就算一个衰减系系数,让value的值等于阈值: 5,10,15
- dropout对小数据防止过拟合有很好的效果,值一般设为0.5,小数据上dropout+sgd在我的大部分实验中，效果提升都非常明显.因此可能的话，建议一定要尝试一下。 dropout的位置比较有讲究, 对于RNN,建议放到输入->RNN与RNN->输出的位置.关于RNN如何用dropout,可以参考这篇论文:http://arxiv.org/abs/1409.2329
- adam,adadelta等,在小数据上,我这里实验的效果不如sgd, sgd收敛速度会慢一些，但是最终收敛后的结果，一般都比较好。如果使用sgd的话,可以选择从1.0或者0.1的学习率开始,隔一段时间,在验证集上检查一下,如果cost没有下降,就对学习率减半. 我看过很多论文都这么搞,我自己实验的结果也很好. 当然,也可以先用ada系列先跑,最后快收敛的时候,更换成sgd继续训练.同样也会有提升.据说adadelta一般在分类问题上效果比较好，adam在生成问题上效果比较好。
- 除了gate之类的地方,需要把输出限制成0-1之外,尽量不要用sigmoid,可以用tanh或者relu之类的激活函数.1. sigmoid函数在-4到4的区间里，才有较大的梯度。之外的区间，梯度接近0，很容易造成梯度消失问题。2. 输入0均值，sigmoid函数的输出不是0均值的。
- rnn的dim和embdding size,一般从128上下开始调整. batch size,一般从128左右开始调整.batch size合适最重要,并不是越大越好.
- word2vec初始化,在小数据上,不仅可以有效提高收敛速度,也可以可以提高结果.
- 尽量对数据做shuffle
- LSTM 的forget gate的bias,用1.0或者更大的值做初始化,可以取得更好的结果,来自这篇论文:http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf, 我这里实验设成1.0,可以提高收敛速度.实际使用中,不同的任务,可能需要尝试不同的值.
- Batch Normalization据说可以提升效果，不过我没有尝试过，建议作为最后提升模型的手段，参考论文：Accelerating Deep Network Training by Reducing Internal Covariate Shift
- 如果你的模型包含全连接层（MLP），并且输入和输出大小一样，可以考虑将MLP替换成Highway Network,我尝试对结果有一点提升，建议作为最后提升模型的手段，原理很简单，就是给输出加了一个gate来控制信息的流动，详细介绍请参考论文: [http://arx](http://arxiv.org/abs/1505.00387)

## Ensemble

Ensemble是论文刷结果的终极核武器,深度学习中一般有以下几种方式

- 同样的参数,不同的初始化方式
- 不同的参数,通过cross-validation,选取最好的几组
- 同样的参数,模型训练的不同阶段，即不同迭代次数的模型。
- 不同的模型,进行线性融合. 例如RNN和传统模型.




