# 混合精度训练
[参考网址](https://mp.weixin.qq.com/s/zBtpwrQ5HtI6uzYOx5VsCQ)
**论文《MIXED PRECISION TRAINING》**

[toc]

## 理论原理
训练过神经网络的小伙伴都知道，神经网络的参数和中间结果绝大部分都是单精度浮点数（即float32）存储和计算的，当网络变得超级大时，降低浮点数精度，比如使用半精度浮点数，显然是提高计算速度，降低存储开销的一个很直接的办法。然而副作用也很显然，如果我们直接降低浮点数的精度直观上必然导致模型训练精度的损失

### 权重备份(master weights)
我们知道半精度浮点数（float16）在计算机中的表示分为1bit的符号位，5bits的指数位和10bits的尾数位，所以它能表示的最小的正数即2^-24（也就是精度到此为止了）。当神经网络中的梯度灰常小的时候，网络训练过程中每一步的迭代（灰常小的梯度 ✖ 也黑小的learning rate）会变得更小，小到float16精度无法表示的时候，相应的梯度就无法得到更新。

解决方法就是前向传播和梯度计算都用float16，但是存储网络参数的梯度时要用float32


## 损失放缩（loss scaling）
于是**loss scaling**方法来了。首先作者统计了一下训练过程中激活函数梯度的分布情况，由于网络中的梯度往往都非常小，导致在使用FP16的时候右边有大量的范围是没有使用的。这种情况下， 我们可以通过放大loss来把整个梯度右移，减少因为精度随时变为0的梯度。

那么问题来了，怎么合理的放大loss呢？一个最简单的方法是常数缩放，把loss一股脑统一放大S倍。float16能表示的最大正数是2^15*(1+1-2^-10)=65504，我们可以统计网络中的梯度，计算出一个常数S，使得最大的梯度不超过float16能表示的最大整数即可。


我们先初始化一个很大的S，如果梯度溢出，我们就把S缩小为原来的二分之一；如果在很多次迭代中梯度都没有溢出，我们也可以尝试把S放大两倍。以此类推，实现动态的loss scaling。

![](https://cdn.jsdelivr.net/gh/donttal/figurebed/img/动态调整.png)
 

## 三大深度学习框架的打开方式
看完了硬核技术细节之后，我们赶紧来看看代码实现吧！如此强大的混合精度训练的代码实现不要太简单了吧😮

### Pytorch
导入Automatic Mixed Precision (AMP)，不要998不要288，只需3行无痛使用！
```python
from apex import amp
model, optimizer = amp.initialize(model, optimizer, opt_level="O1") # 这里是“欧一”，不是“零一”
with amp.scale_loss(loss, optimizer) as scaled_loss:
    scaled_loss.backward()
```
来看个例子，将上面三行按照正确的位置插入到自己原来的代码中就可以实现酷炫的半精度训练啦！
```python
import torch
from apex import amp
model = ... 
optimizer = ...

#包装model和optimizer
model, optimizer = amp.initialize(model, optimizer, opt_level="O1")

for data, label in data_iter: 
    out = model(data) 
    loss = criterion(out, label) 
    optimizer.zero_grad() 
    
    #loss scaling，代替loss.backward()
    with amp.scaled_loss(loss, optimizer) as scaled_loss:   
        scaled_loss.backward() 
optimizer.step()
```

### Tensorflow
一句话实现混合精度训练之修改环境变量，在python脚本中设置环境变量
```python
os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'
```
#### Keras-based示例
```python
opt = tf.keras.optimizers.Adam()

# add a line
opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(
            opt,
            loss_scale='dynamic')
            
model.compile(loss=loss, optimizer=opt)
model.fit(...)
```