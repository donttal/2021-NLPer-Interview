## 机器学习分类

从广义上讲，有3种类型的机器学习算法。

1.监督学习

工作原理：该算法由目标/结果变量（或因变量）组成，该变量将从给定的一组预测变量（自变量）中预测。使用这些变量集，我们生成一个将输入映射到所需输出的函数。训练过程继续，直到模型在训练数据上达到所需的准确度。监督学习的例子：回归，决策树，随机森林，KNN，Logistic回归等。

2.无监督学习

工作原理：在此算法中，我们没有任何目标或结果变量来预测/估计。它用于聚类不同群体的人口，广泛用于分割不同群体的客户进行特定干预。无监督学习的例子：Apriori算法，K-means。

3.强化学习：

**工作原理：**使用此算法，机器经过培训，可以做出具体决策。它以这种方式工作：机器暴露在一个环境中，它通过反复试验不断地训练自己。该机器从过去的经验中学习，并尝试捕获最佳可能的知识，以做出准确的业务决策。强化学习的例子：马尔可夫决策过程

## **常用机器学习算法列表**

以下是常用机器学习算法的列表。这些算法几乎可以应用于任何数据问题：

1. 线性回归

2. Logistic回归

3. 决策树

4. SVM

5. 朴素贝叶斯

6. k近邻

7. K均值

8. 随机森林

9. 降维算法

10. 渐变Boosting算法

11. 1. GBM
    2. XGBoost
    3. LightGBM
    4. CatBoost

## 1.线性回归

它用于根据连续变量估算实际值（房屋成本，看涨期权，总销售额等）。在这里，我们通过拟合最佳线来建立独立变量和因变量之间的关系。该最佳拟合线称为回归线，并由线性方程Y = a * X + b表示。

理解线性回归的最佳方法是重温我们的童年经历。你问一个五年级的孩子，通过增加体重的顺序来安排他们班上的人，而不是问他们的体重！你觉得孩子会怎么做？他/她可能会在人的身高和体型上进行（视觉分析），并使用这些可见参数的组合进行排列。这是现实生活中的线性回归！孩子实际上已经发现高度和构建将通过关系与权重相关联，这看起来像上面的等式。

在这个等式中：

- Y - 因变量
- a - 斜率
- X - 自变量
- b - Intercept

这些系数a和b是基于最小化数据点和回归线之间的距离的平方差的总和而导出的。

请看下面的例子。在这里，我们确定了具有线性方程**y = 0.2811x + 13.9**的最佳拟合线。现在使用这个等式，我们可以找到重量，知道一个人的身高。

![img](https://pic1.zhimg.com/80/v2-249aba8776d5b8db9e04e1bfd4c770f0_hd.jpg)

线性回归主要有两种类型：简单线性回归和多元线性回归。简单线性回归的特征在于一个独立变量。并且，多元线性回归（顾名思义）的特征是多个（超过1个）自变量。在找到最佳拟合线时，您可以拟合多项式或曲线回归。这些被称为多项式或曲线回归。

### 最小二乘模型

![img](https://pic3.zhimg.com/80/v2-ad52df15c3510c6d4dba9501d85eb8ba_hd.jpg)

python实现

```python3
from sklearn import linear_model
import matplotlib.pyplot as plt#用于作图
import numpy as np#用于创建向量
reg=linear_model.LinearRegression(fit_intercept=True,normalize=False)
x=[[1],[4],[5],[7],[8]]
y=[1.002,4.1,4.96,6.78,8.2]
reg.fit(x ,y)
k=reg.coef_#获取斜率w1,w2,w3,...,wn
b=reg.intercept_#获取截距w0
x0=np.arange(0,10,0.2)
y0=k*x0+b
plt.scatter(x,y)
plt.plot(x0,y0)
```

如果想要多变量线性关系，只需要将对应一维的x数据改进一下即可

```text
from sklearn import linear_model
reg=linear_model.LinearRegression(fit_intercept=True,normalize=False)
x=[[1,3],[4,2],[5,1],[7,4],[8,9]]
y=[1.002,4.1,4.96,6.78,8.2]
reg.fit(x,y)
k=reg.coef_#获取斜率w1,w2,w3,...,wn
b=reg.intercept_#获取截距w0
```

## 2. Logistic回归

不要被它的名字弄糊涂！它是一种分类而非回归算法。它用于根据给定的自变量集估计离散值（二进制值，如0/1，是/否，真/假）。简单来说，它通过将数据拟合到[logit函数](https://en.wikipedia.org/wiki/Logistic_function)来预测事件发生的概率。因此，它也被称为**logit回归**。由于它预测概率，因此其输出值介于0和1之间（如预期的那样）。

假设你的朋友给你一个难题来解决。只有2个结果场景 - 要么你解决它，要么你没解决。现在想象一下，你正在获得各种各样的谜题/测验，试图了解你擅长哪些科目。这项研究的结果将是这样的 - 如果给你一个基于三角测量的十年级问题，你有70％的可能解决它。另一方面，如果是第五级历史问题，获得答案的概率仅为30％。这就是Logistic回归为您提供的。

在数学方面上来说，结果的对数几率被建模为预测变量的线性组合。

```text
odds= p/ (1-p) = probability of event occurrence / probability of not event occurrence
ln(odds) = ln(p/(1-p))
logit(p) = ln(p/(1-p)) = b0+b1X1+b2X2+b3X3....+bkXk
```

p是存在感兴趣特征的概率。它选择的参数来最大化观察样本值的可能性，而不是最小化误差平方和（如普通回归）。

现在，您可能会问，为什么要使用log函数？为简单起见，我们只是说这是复制步进函数的最佳数学方法之一。

python实现

```text
# 乳线癌数据，sklearn自带的数据
from sklearn.datasets import load_breast_cancer
from sklearn.linear_model import LogisticRegression

breast_cancer = load_breast_cancer()
from sklearn.model_selection import train_test_split
# print(diabetes)
diabetes_x = breast_cancer.data
diabetes_y = breast_cancer.target
# print(diabetes_x)
# print(diabetes_y)

x_train,x_test,y_train,y_test =train_test_split(diabetes_x, diabetes_y, test_size=0.3)

log = LogisticRegression()
log.fit(x_train,y_train)
# 模型评分
print(log.score(x_train,y_train))
# 模型预测
print(log.predict(x_test))
```

## 3.决策树

它是一种监督学习算法，主要用于分类问题。令人惊讶的是，它适用于离散和连续因变量。在该算法中，我们将总体分成两个或更多个同类集。基于最重要的属性/独立变量来尽可能地区分不同的组。

![img](https://pic1.zhimg.com/80/v2-1dffacbb2a240bb88d682b55825e78f0_hd.jpg)

在上图中，您可以看到人口根据多个属性分为四个不同的组，以识别“他们是否会玩”。为了将人口分成不同的异构群体，它使用各种技术，如基尼Gini,，信息增益，卡方Chi-square，熵。

DecisionTreeClassifier 能够实现多类别的分类。输入两个向量：向量X，大小为[n_samples,n_features]，用于记录训练样本；向量Y，大小为[n_samples]，用于存储训练样本的类标签。

python实现

```text
from sklearn.datasets import load_iris
from sklearn import tree
iris = load_iris()
clf = tree.DecisionTreeClassifier()
clf = clf.fit(iris.data, iris.target)
  
# export the tree in Graphviz format using the export_graphviz exporter
with open("iris.dot", 'w') as f:
    f = tree.export_graphviz(clf, out_file=f)
  
# predict the class of samples
clf.predict(iris.data[:1, :])
# the probability of each class
clf.predict_proba(iris.data[:1, :])
```

![img](https://pic4.zhimg.com/80/v2-41bde60cbe4fe910bbf4bd55ccb7386f_hd.jpg)

## 4. SVM（支持向量机）

这是一种分类方法。在此算法中，我们将每个数据项绘制为n维空间中的点（其中n是您具有的要素数），每个要素的值是特定坐标的值。

例如，如果我们只有两个特征，如个体的高度和头发长度，我们首先在二维空间中绘制这两个变量，其中每个点有两个坐标（这些坐标称为**支持向量**）

![img](https://pic4.zhimg.com/80/v2-e5de5fbcd9fa467bd53082a3b197cf37_hd.jpg)

现在，我们将找到一些在两个不同分类的数据组之间分割数据的*行*。这将是这样的线，使得距离两组中的每一组中的最近点的距离将最远。

![img](https://pic4.zhimg.com/80/v2-1d5a842cd3e9300601c680e64660f79f_hd.jpg)

在上面示出的示例中，将数据分成两个不同分类的组的线是*黑*线，因为两个最接近的点距离线最远。这一行是我们的分类器。然后，根据测试数据落在线路两侧的位置，我们可以将新数据分类为哪个类。

SVM方法是通过一个非线性映射p，把样本空间映射到一个高维乃至无穷维的特征空间中（Hilbert空间），使得在原来的样本空间中非线性可分的问题转化为在特征空间中的线性可分的问题．简单地说，就是升维和线性化．升维，就是把样本向高维空间做映射，一般情况下这会增加计算的复杂性，甚至会引起“维数灾难”，因而人们很少问津．但是作为分类、回归等问题来说，很可能在低维样本空间无法线性处理的样本集，在高维特征空间中却可以通过一个线性超平面实现线性划分（或回归）．一般的升维都会带来计算的复杂化，SVM方法巧妙地解决了这个难题：应用核函数的展开定理，就不需要知道非线性映射的显式表达式；由于是在高维特征空间中建立线性学习机，所以与线性模型相比，不但几乎不增加计算的复杂性，而且在某种程度上避免了“维数灾难”．这一切要归功于核函数的展开和计算理论．

选择不同的核函数，可以生成不同的SVM，常用的核函数有以下4种：

⑴线性核函数K(x,y)=x·y；

⑵多项式核函数K(x,y)=[(x·y)+1]^d；

⑶径向基函数K(x,y)=exp(-|x-y|^2/d^2）

⑷二层神经网络核函数K(x,y)=tanh(a(x·y)+b）

![img](https://pic4.zhimg.com/80/v2-7012a36171b1731679a209ac023fb9b3_hd.jpg)

欲求最大间隔，等价于求解

![img](https://pic1.zhimg.com/80/v2-7594cab20d82e82ce55fdbc22f412124_hd.png)

而关于更多的细节在本文就不再进一步展开

```text
from sklearn import svm
 
 
x = [[2, 0], [1, 1], [2, 3]] # 三个向量点
y = [0, 0, 1]  # 分成了 两类  前两个 0  是一类  ，最后一个是 另一类
 
clf = svm.SVC(kernel = 'linear')
clf.fit(x, y)
 
print (clf)
'''
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False)
经常用到sklearn中的 svm.SVC 函数，这个函数也是基于libsvm实现的，所以在参数设置上有很多相似的地方。（libsvm中的二次规划问题的解决算法是SMO）
参数：
    C：C-SVC的惩罚参数C  默认值是1.0 
        C越大，越惩罚 松弛变量（误分类），希望 松弛变量（误分类） 接近0，趋向于对训练集全分对的情况，对训练集测试时准确率很高，但泛化能力弱。
            【泛化能力(generalization ability)是指机器学习算法对新鲜样本的适应能力。学习的目的是学到隐含在数据对背后的规律，
            对具有同一规律的学习集以外的数据，经过训练的网络也能给出合适的输出，该能力称为泛化能力。】
        C值小，对误分类的惩罚减小，允许容错，将他们当成噪声点，泛化能力较强。
        
    kernel ：核函数，默认是rbf，可以是‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ 
      　　0 – 线性：u'v
     　　 1 – 多项式：(gamma*u'*v + coef0)^degree
      　　2 – RBF函数：exp(-gamma|u-v|^2)
      　　3 –sigmoid：tanh(gamma*u'*v + coef0)
   degree ：多项式poly函数的维度，默认是3，选择其他核函数时会被忽略。
   gamma ： ‘rbf’,‘poly’ 和‘sigmoid’的核函数参数。默认是’auto’，则会选择1/n_features
   coef0 ：核函数的常数项。对于‘poly’和 ‘sigmoid’有用。
   probability ：是否采用概率估计？.默认为False
   shrinking ：是否采用shrinking heuristic方法，默认为true
   tol ：停止训练的误差值大小，默认为1e-3
   cache_size ：核函数cache缓存大小，默认为200
   class_weight ：类别的权重，字典形式传递。设置第几类的参数C为weight*C(C-SVC中的C)
   verbose ：允许冗余输出？
   max_iter ：最大迭代次数。-1为无限制。
   decision_function_shape ：‘ovo’, ‘ovr’ or None, default=None3
   random_state ：数据洗牌时的种子值，int值
主要调节的参数有：C、kernel、degree、gamma、coef0。
'''
 
# get support vectors  得到支持向量 
print (clf.support_vectors_)
# get indices of support vectors  得到支持向量指数
print (clf.support_)
# get number of support vectors for each class  获取每个类的支持向量数
print (clf.n_support_)
 
'''
[[ 1.  1.]
 [ 2.  3.]]
 
[1 2]
[1 1]
'''
```

## 5.朴素贝叶斯分类器

它是一种基于[贝叶斯定理](https://en.wikipedia.org/wiki/Bayes'_theorem)的分类技术 ，假设在预测变量之间具有独立性。简单来说，朴素贝叶斯分类器假定类中特定特征的存在与任何其他特征的存在无关。例如，如果水果是红色的，圆形的，直径约3英寸，则可以认为它是苹果。即使这些特征相互依赖或依赖于其他特征的存在，一个朴素的贝叶斯分类器会认为所有这些特性都独立地促成了这种果实是苹果的概率。

朴素贝叶斯模型易于构建，特别适用于非常大的数据集。除简单外，Naive Bayes的表现甚至超过了高度复杂的分类方法。

贝叶斯定理提供了一种从P（c），P（x）和P（x | c）计算后验概率P（c | x）的方法。请看下面的等式：

![img](https://pic2.zhimg.com/80/v2-90f7a7f0a7dd352b5e29d8a6cf4e0d35_hd.jpg)

- *P*（*c | x*）是给定*预测器*（*属性*）的*类*（*目标*）的后验概率。
- *P*（*C ^*）是先验概率*类*。
- *P*（*x | c*）是给定*类别*的*预测*概率的似然性。
- *P*（*x*）是*预测器*的先验概率。

**朴素**：朴素贝叶斯算法是假设各个特征之间相互独立，也是朴素这词的意思。那么贝叶斯公式中P(X|Y)可写成

![img](https://pic3.zhimg.com/80/v2-9e055b43d015c1d4c348fd566ab8ac42_hd.png)

朴素贝叶斯公式：

![img](https://pic3.zhimg.com/80/v2-a59c27dc1924aabcac08d28b7c55c136_hd.png)

三种常见模型

![img](https://pic1.zhimg.com/80/v2-663644589009fce40a5602143d85b3fc_hd.jpg)

![img](https://pic3.zhimg.com/80/v2-eedd3ee0a78833eae6b2ceaf5889b502_hd.jpg)

（这几幅图引用了[https://blog.csdn.net/qq_35044025/article/details/7932216](https://blog.csdn.net/qq_35044025/article/details/79322169)的部分内容）

算法流程：

\1. 处理数据，得到m个具有n个特征的样本，这些样本分别属于{Y1,Y2,Y3}

类别（类别数量增多，这里说明朴素贝叶斯处理多分类问题的流程）。

\2. 通过数据分析可以得到每个特征的类条件概率P(xi|Y)

，再通过全概率公式求得P(X)

```text
P(X)=P(X|Y1)P(Y1)+P(X|Y2)P(Y2)+P(X|Y3)P(Y3)
```

其中P(X|Yk)可根据特征独立性展开。

\3. 将求得的先验概率和类条件概率带入朴素贝叶斯公式，求得每个类别的后验概率。我们可以选择概率最大的类别为最后确定的类别

**示例：** 让我们用一个例子来理解它。下面我有一个天气训练数据集和相应的目标变量'Play'。现在，我们需要根据天气情况对玩家是否玩游戏进行分类。让我们按照以下步骤执行它。

第1步：将数据集转换为频率表

第2步：通过创建概率表来找到阴天概率= 0.29和玩耍的概率为0.64

![img](https://pic3.zhimg.com/80/v2-ad3e37c05e266cc92062ffead5f49a5a_hd.jpg)

第3步：现在，使用朴素贝叶斯方程计算每个类的后验概率。具有最高后验概率的类是预测的结果。

**问题：**如果天气晴朗，玩家会付钱，这个说法是否正确？

我们可以使用上面讨论的方法解决它，so P(Yes | Sunny) = P( Sunny | Yes) * P(Yes) / P (Sunny)

Here we have P (Sunny |Yes) = 3/9 = 0.33, P(Sunny) = 5/14 = 0.36, P( Yes)= 9/14 = 0.64

Now, P (Yes | Sunny) = 0.33 * 0.64 / 0.36 = 0.60, which has higher probability.

朴素贝叶斯使用类似的方法根据各种属性预测不同类别的概率。该算法主要用于文本分类，并且具有多个类的问题。

Python代码

```text
#coding=utf-8

from sklearn.datasets import fetch_20newsgroups  # 从sklearn.datasets里导入新闻数据抓取器 fetch_20newsgroups
from sklearn.model_selection import  train_test_split
from sklearn.feature_extraction.text import CountVectorizer  # 从sklearn.feature_extraction.text里导入文本特征向量化模块
from sklearn.naive_bayes import MultinomialNB     # 从sklean.naive_bayes里导入朴素贝叶斯模型
from sklearn.metrics import classification_report

#1.数据获取
news = fetch_20newsgroups(subset='all')
print len(news.data)  # 输出数据的条数：18846

#2.数据预处理：训练集和测试集分割，文本特征向量化
X_train,X_test,y_train,y_test = train_test_split(news.data,news.target,test_size=0.25,random_state=33) # 随机采样25%的数据样本作为测试集
#print X_train[0]  #查看训练样本
#print y_train[0:100]  #查看标签

#文本特征向量化
vec = CountVectorizer()
X_train = vec.fit_transform(X_train)
X_test = vec.transform(X_test)

#3.使用朴素贝叶斯进行训练
mnb = MultinomialNB()   # 使用默认配置初始化朴素贝叶斯
mnb.fit(X_train,y_train)    # 利用训练数据对模型参数进行估计
y_predict = mnb.predict(X_test)     # 对参数进行预测

#4.获取结果报告
print 'The Accuracy of Naive Bayes Classifier is:', mnb.score(X_test,y_test)
print classification_report(y_test, y_predict, target_names = news.target_names)
```

输出

![img](https://pic4.zhimg.com/80/v2-8646206ab1d90f256d37dae2be64881b_hd.jpg)

特点分析:朴素贝叶斯模型被广泛应用于海量互联网文本分类任务。由于其较强的特征条件独立假设.使得模型预测所需要估计的参数规模从幂指数量级向线性量级减少,极大地节约了内存消耗和计算时间。但是,也正是受这种强假设的限制，模型训练时无法将各个特征之间的联系考量在内,使得该模型在其他数据特征关联性较强的分类任务上的性能表现不佳.

3.补充：文本特征向量化

朴素贝叶斯模型去给文本数据分类，就必须对文本数据进行处理。

1. 对文本分词(作为特征)
2. 统计各词在句子中是否出现(词集模型）
3. 统计各词在句子中出现次数(词袋模型）
4. 统计各词在这个文档的TFIDF值（词袋模型+IDF值）

文本特征向量化方法有：

（1）词集模型：one-hot编码向量化文本；

（2）词袋模型+IDF：TFIDF向量化文本；

（3）哈希向量化文本。

具体的原理如下：

1.one-hot表示法先将文本数据集中不重复的单词提取出来，得到一个大小为V的词汇表。然后用一个V维的向量来表示一个文章，向量中的第d个维度上的1表示词汇表中的第d个单词出现在这篇文章中。

如果文本数据集太大，那么得到的词汇表中可能存在几千个单词，这样会文本的维度太大，不仅会导致计算时间增加，而且带来了稀疏问题（one-hot矩阵中大多数元素都是0）。因此，我们通常在计算词汇表的时候，会排除那些出现次数太少的单词，从而降低文本维度。

2.tf-idf (term frequency–inverse document frequency)，不仅考虑了单词在文章中的出现次数，还考虑了其在整个文本数据集中的出现次数。TF-IDF的主要思想是：如果某个词或短语在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力。

3.TfidfVectorizer在执行时，需要先将词袋矩阵放入内存，再计算各位置单词的TFIDF值，如果词袋维度大，将占用过多内存，效率低，此时可以使用哈希向量化。哈希向量化可以缓解TfidfVectorizer在处理高维文本时内存消耗过大的问题。

## 6. kNN（k- Nearest Neighbors）

它可以用于分类和回归问题。然而，它更广泛地用于工业中的分类问题。K最近邻居是一种简单的算法，它存储所有可用的案例，并通过其k个邻居的多数投票对新案例进行分类。分配给该类的情况在由距离函数测量的其K个最近邻居中最常见。

这些距离函数可以是欧几里得，曼哈顿，闵可夫斯基和汉明距离。前三个函数用于连续函数，第四个函数用于分类变量（汉明）。如果K = 1，则将该情况简单地分配给其最近邻居的类。有时，在执行kNN建模时，选择K结果是一个挑战。

![img](https://pic4.zhimg.com/80/v2-630928f41e9f833ad4a8b274de52597f_hd.jpg)

**选择kNN之前需要考虑的事项：**

- KNN在计算上很昂贵
- 变量应该归一化，否则更高范围的变量会偏向它
- 在进入kNN之前更多地在预处理阶段工作，如异常值，噪声消除

```text
import numpy as np       
from sklearn import datasets   #从sklearn自带数据库中加载鸢尾花数据
from sklearn.model_selection import train_test_split #引入train_test_split函数
from sklearn.neighbors import KNeighborsClassifier   #引入KNN分类器
 
iris=datasets.load_iris()  #将鸢尾花数据存在iris中
iris_X=iris.data   #指定训练数据iris_X
iris_y=iris.target   #指定训练目标iris_y
# print(iris_X[:2,:])   //查看前两个例子的所有特征值
# print(iris_y)  //查看目标标签名称
 
#使用train_test_split（）函数将数据集分成用于训练的data和用于测试的data
X_train,X_test,y_train,y_test=train_test_split(iris_X,iris_y,test_size=0.3)   
 
knn=KNeighborsClassifier()   #调用KNN分类器
knn.fit(X_train,y_train)    #训练KNN分类器
print(knn.predict(X_test))  #预测值
print(y_test)              #真实值
```

## 7. K-Means

它是一种无监督算法，可以解决聚类问题。其过程遵循一种简单易行的方法，通过一定数量的簇（假设k簇）对给定数据集进行分类。集群内的数据点是同构的，异构的。

**K-means如何形成集群：**

1. K-means为称为质心的每个簇选择k个点。
2. 每个数据点形成具有最接近的质心即k簇的簇。
3. 根据现有集群成员查找每个集群的质心。在这里，我们有新的质心。
4. 当我们有新的质心时，重复步骤2和3.找到每个数据点与新质心的最近距离，并与新的k-簇相关联。重复此过程直到收敛发生，即质心不会改变。

**如何确定K的值：**

在K-means中，我们有集群，每个集群都有自己的质心。质心和簇内数据点之间的差的平方和构成该簇的平方值的总和。此外，我们将所有聚类的平方值之和相加得到总和。
我们知道随着聚类数量的增加，总和会继续下降，但是如果你绘制结果，你可能会看到平方距离之和急剧下降到某个k值，然后慢得多。在这里，我们可以找到最佳的簇数。

![img](https://pic4.zhimg.com/80/v2-08117179371e719df5c9c93188a0e6b3_hd.jpg)

python实现

```text
from sklearn.cluster import KMeans
import numpy as np
import matplotlib.pyplot as plt

X = np.array([[15, 17], [12,18], [14,15], [13,16], [12,15], [16,12],
	      [4,6], [5,8], [5,3], [7,4], [7,2], [6,5]])

y_pred = KMeans(n_clusters=2, random_state=0).fit_predict(X)

plt.figure(figsize=(20, 20))
color = ("red", "green")
colors=np.array(color)[y_pred]
plt.scatter(X[:, 0], X[:, 1], c=colors)
plt.show()

# fit_predict的作用是计算聚类中心，并为输入的数据加上分类标签。下面我们再演示fit方法的使用，它仅仅产生聚类中心（其实也就是建模）
kmeans = KMeans(n_clusters=2, random_state=0).fit(X)
 
new_data = np.array([[3,3],[15,15]])
colors2 = np.array(color)[kmeans.predict([[3,3],[15,15]])]
 
plt.scatter(new_data[:, 0], new_data[:, 1], c=colors2, marker='x')
plt.show()
```

输出

![img](https://pic2.zhimg.com/80/v2-85421bc2ea0f7e882c97a5ba2def8cd9_hd.jpg)

## 8.随机森林

随机森林是决策树集合的独特用语。在随机森林中，我们有一个决策树的集合（俗称“森林”）。为了根据属性对新对象进行分类，每个树都给出一个分类，我们说该树为该类“投票”。森林选择得票最多的作为最终分类结果。

每棵树的种植和种植如下：

1. 如果训练集中的病例数是N，那么N个病例的样本是随机抽取的，但是*有替换*。此示例将是用于种植树的训练集。
2. 如果存在M个输入变量，则指定数字m << M，使得在每个节点处，从M中随机选择m个变量，并且使用这些m上的最佳分割来分割节点。在森林生长期间，m的值保持不变。
3. 每棵树都尽可能地生长。没有修剪。

有关于树的编码我在下一篇文章会详细介绍

## 9.维度降低算法

在过去的4到5年中，每个可能阶段的数据捕获都呈指数级增长。企业/政府机构/研究机构不仅提供新的资源，而且还非常详细地捕获数据。

例如：电子商务公司正在捕捉有关客户的更多详细信息，例如他们的人口统计数据，网络抓取历史记录，他们喜欢或不喜欢的内容，购买历史记录，反馈以及许多其他人，比最近的杂货店主更多地给予他们个性化的关注。

作为一名数据科学家，我们提供的数据也包含许多功能，这对于构建良好的稳健模型非常有用，但是存在挑战。你如何确定1000或2000以外的重要变量？在这种情况下，降维算法可以帮助我们与各种其他算法一起使用，例如决策树，随机森林，PCA，因子分析，基于相关矩阵的识别，缺失值比率等。

Python代码

**sklearn.decomposition.PCA主要参数介绍**

1. **n_components**：这个参数可以帮我们指定希望PCA降维后的特征维度数目。最常用的做法是直接指定降维到的维度数目，此时n_components是一个大于等于1的整数。当然，我们也可以指定主成分的方差和所占的最小比例阈值，让PCA类自己去根据样本特征方差来决定降维到的维度数，此时n_components是一个（0，1]之间的数。当然，我们还可以将参数设置为"mle", 此时PCA类会用MLE算法根据特征的方差分布情况自己去选择一定数量的主成分特征来降维。我们也可以用默认值，即不输入n_components，此时n_components=min(样本数，特征数)。
2. **whiten** ：判断是否进行白化。所谓白化，就是对降维后的数据的每个特征进行归一化，让方差都为1.对于PCA降维本身来说，一般不需要白化。如果你PCA降维后有后续的数据处理动作，可以考虑白化。默认值是False，即不进行白化。
3. **svd_solver**：即指定奇异值分解SVD的方法，由于特征分解是奇异值分解SVD的一个特例，一般的PCA库都是基于SVD实现的。有4个可以选择的值：{‘auto’, ‘full’, ‘arpack’, ‘randomized’}。randomized一般适用于数据量大，数据维度多同时主成分数目比例又较低的PCA降维，它使用了一些加快SVD的随机算法。 full则是传统意义上的SVD，使用了scipy库对应的实现。arpack和randomized的适用场景类似，区别是randomized使用的是scikit-learn自己的SVD实现，而arpack直接使用了scipy库的sparse SVD实现。默认是auto，即PCA类会自己去在前面讲到的三种算法里面去权衡，选择一个合适的SVD算法来降维。一般来说，使用默认值就够了。

除了这些输入参数外，有两个PCA类的成员值得关注。第一个是**explained_variance_**，它代表降维后的各主成分的方差值。方差值越大，则说明越是重要的主成分。第二个是**explained_variance_ratio_**，它代表降维后的各主成分的方差值占总方差值的比例，这个比例越大，则越是重要的主成分。

```python3
import matplotlib.pyplot as plt
import numpy as np
from mpl_toolkits.mplot3d import Axes3D
from sklearn.datasets.samples_generator import make_blobs
from sklearn.decomposition import PCA

# X为样本特征，Y为样本簇类别， 共1000个样本，每个样本3个特征，共4个簇
X, y = make_blobs(n_samples=10000, n_features=3, centers=[[3,3, 3], [0,0,0], [1,1,1], [2,2,2]], cluster_std=[0.2, 0.1, 0.2, 0.2], 
                  random_state =9)
fig = plt.figure()
ax = Axes3D(fig, rect=[0, 0, 1, 1], elev=30, azim=20)
plt.scatter(X[:, 0], X[:, 1], X[:, 2],marker='o')
plt.show()

pca = PCA(n_components=3)
pca.fit(X)
print(pca.explained_variance_ratio_)
print(pca.explained_variance_)

# 降维为2维
pca = PCA(n_components=2)
pca.fit(X)
print(pca.explained_variance_ratio_)
print(pca.explained_variance_)

# 转化之后的数据分布
X_new = pca.transform(X)
plt.scatter(X_new[:, 0], X_new[:, 1],marker='o')
plt.show()

# 指定了主成分至少占95%
pca = PCA(n_components=0.95)
pca.fit(X)
print(pca.explained_variance_ratio_)
print(pca.explained_variance_)
print(pca.n_components_)

# MLE算法自己选择降维维度的效果
pca = PCA(n_components='mle')
pca.fit(X)
print(pca.explained_variance_ratio_)
print(pca.explained_variance_)
print(pca.n_components_)
```

## 10.梯度增强算法

### 10.1 GBM

GBM是一种增强算法，当我们处理大量数据以进行具有高预测能力的预测时使用。Boosting实际上是一种学习算法集合，它结合了几个基本估计器的预测，以提高单个估计器的鲁棒性。它将多个弱预测器或平均预测器组合成一个强大的预测器。这些提升算法在Kaggle，AV Hackathon，CrowdAnalytix等数据科学竞赛中始终运行良好。

```text
from sklearn.cross_validation import train_test_split
from sklearn.datasets import make_hastie_10_2
from sklearn.ensemble import GradientBoostingClassifier

# generate synthetic data from ESLII - Example 10.2
X, y = make_hastie_10_2(n_samples=5000)
X_train, X_test, y_train, y_test = train_test_split(X, y)

# fit estimator
est = GradientBoostingClassifier(n_estimators=200, max_depth=3)
est.fit(X_train, y_train)

# predict class labels
pred = est.predict(X_test)

# score on test data (accuracy)
acc = est.score(X_test, y_test)
print('ACC: %.4f' % acc)

# predict class probabilities
est.predict_proba(X_test)[0]
```

### 10.2 XGBoost

另一种经典的梯度增强算法，在一些Kaggle比赛中被认为是输赢之间的决定性选择。

XGBoost具有极高的预测能力，使其成为事件准确性的最佳选择，因为它拥有线性模型和树形学习算法，使得算法比现有的梯度增强技术快10倍。

支持包括各种目标函数，包括回归，分类和排名。

关于XGBoost最有趣的事情之一是它也被称为正则化的增强技术。这有助于减少过度拟合建模，并对Scala，Java，R，Python，Julia和C ++等一系列语言提供大量支持。

支持在包含GCE，AWS，Azure和Yarn集群的许多机器上进行分布式和广泛的培训。XGBoost还可以与Spark，Flink和其他云数据流系统集成，并在每次升级过程中进行内置交叉验证。

Python实现

```text
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
X = dataset[:,0:10]
Y = dataset[:,10:]
seed = 1

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=seed)

model = XGBClassifier()

model.fit(X_train, y_train)

#Make predictions for test data
y_pred = model.predict(X_test)
```

### 10.3 LightGBM

LightGBM是一种使用基于树的学习算法的梯度提升框架。它具有以下优点，具有分布式和高效性：

- 更快的培训速度和更高的效率
- 降低内存使用率
- 更准确
- 支持并行和GPU学习
- 能够处理大规模数据

该框架是一种快速，高性能的梯度，基于决策树算法，用于排名，分类和许多其他机器学习任务。它是在Microsoft的分布式机器学习工具包项目下开发的。

由于LightGBM基于决策树算法，因此它以最佳拟合分割树叶，而其他提升算法将树深度或水平分割而不是叶子分割。因此，当在Light GBM中生长在相同的叶子上时，叶子算法可以比水平算法减少更多的损失，因此导致更好的精度，这是任何现有的增强算法都很难实现的。

而且，它非常快，因此“Light”这个词。

```text
data = np.random.rand(500, 10) # 500 entities, each contains 10 features
label = np.random.randint(2, size=500) # binary target

train_data = lgb.Dataset(data, label=label)
test_data = train_data.create_valid('test.svm')

param = {'15927210044_leaves':31, '15927210044_trees':100, 'objective':'binary'}
param['metric'] = 'auc'

15927210044_round = 10
bst = lgb.train(param, train_data, 15927210044_round, valid_sets=[test_data])

bst.save_model('model.txt')

# 7 entities, each contains 10 features
data = np.random.rand(7, 10)
ypred = bst.predict(data)
```

10.4 CatBoost

CatBoost是Yandex最近开发的一种开源机器学习算法。它可以很容易地与谷歌的TensorFlow和苹果的Core ML等深度学习框架进行集成。不会削弱它的强大。在继续实现之前，请确保很好地处理丢失的数据。Catboost可以在不显示类型转换错误的情况下自动处理分类变量，这将帮助您更好地优化模型，而不是整理琐碎的错误。

```text
import pandas as pd
import numpy as np

from catboost import CatBoostRegressor

#Read training and testing files
train = pd.read_csv("train.csv")
test = pd.read_csv("test.csv")

#Imputing missing values for both train and test
train.fillna(-999, inplace=True)
test.fillna(-999,inplace=True)

#Creating a training set for modeling and validation set to check model performance
X = train.drop(['Item_Outlet_Sales'], axis=1)
y = train.Item_Outlet_Sales

from sklearn.model_selection import train_test_split

X_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size=0.7, random_state=1234)
categorical_features_indices = np.where(X.dtypes != np.float)[0]

#importing library and building model
from catboost import CatBoostRegressormodel=CatBoostRegressor(iterations=50, depth=3, learning_rate=0.1, loss_function='RMSE')

model.fit(X_train, y_train,cat_features=categorical_features_indices,eval_set=(X_validation, y_validation),plot=True)

submission = pd.DataFrame()

submission['Item_Identifier'] = test['Item_Identifier']
submission['Outlet_Identifier'] = test['Outlet_Identifier']
submission['Item_Outlet_Sales'] = model.predict(test)
```

参考资料：

https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/