# 集成学习

tags: 机器学习

---

https://www.jiqizhixin.com/articles/2018-07-28-3

## 集成学习一览

**组合多个弱监督模型以得到一个更好更全面的强监督模型，其思想在于：即便某一个弱分类器得到了错误的预测，其他的弱分类器也可以将错误纠正回来。**

集成方法奏效的原因是不同的模型**通常不会**在测试集上产生相同的误差。集成模型能至少与它的任一成员表现得一样好。**如果成员的误差是独立的**，集成将显著提升模型的性能。

学习策略推荐：

> - 数据集大： 划分成多个小数据集，学习多个模型进行组合。
> - 数据集小： 利用 Bootstrap 方法进行抽样，得到多个数据集，分别训练多个模型再进行组合。

## 1. Bagging

代表模型： **随机森林， Bagging meta-estimator**

先通过采样构造 k 个不同的数据集，学习得到k 个基学习器， 基学习器之间不存在依赖关系，可同时生成。    

更具体的，如果采样所得的训练集与原始数据集大小相同，那所得数据集中大概有原始数据集 `2/3` 的实例

- **Bootstrap：**  一种有放回的抽样方法，目的是为了得到统计量的分布以及置信空间。

  > 1. 采用有放回抽样方法从原始样本中抽取一定数量的样本
  > 2. 根据抽出的样本计算想要得到的统计量T
  > 3. 重复上述N次（一般大于1000），得到N个统计量T
  > 4. 根据这N个统计量，即可计算出统计量的置信区间

- **Bagging 的基本思路：**

  > 1.  利用**Bootstrap**对训练集随机采样，重复进行 `T` 次
  > 2. 基于每个采样集训练一个弱学习器，得到 T 个弱学习器
  > 3. 预测时，分类问题采用投票方式， 回归问题采用 N 个模型预测平均方式。

## 2.  Boosting 

代表模型：**AdaBoost， XGBoost， GBDT， Light GBM， CatBoost**

学习一系列弱学习器，然后组合成一个强学习器。基于**串行策略**：弱学习器之间存在依赖关系，新的学习器需要根据上一个学习器生成。

- Boosting基本思路：

  > 1. 先从初始训练集训练一个弱学习器，初始训练集各个样本权重相同
  >
  > 2. 根据上一个弱学习器的表现，调整样本权重，是的分类错误的样本得到更多关注
  > 3. 基于调整后的样本分布，训练下一个弱学习器、
  > 4. 测试时，对各基学习器**加权**得到最终结果
  

## 3. Stacking

训练一个模型用于组合其他各个模型。首先我们先训练多个不同的模型，然后把之前训练的各个模型的输出为输入来训练一个模型，以得到一个最终的输出。

- Stacking 基本思路：首先我们先训练多个不同的模型，然后把之前训练的各个模型的输出为输入来训练一个模型，以得到一个最终的输出。

  > - 先从初始训练集训练 `T` 个**不同的初级学习器**;
  > - 利用每个初级学习器的**输出**构建一个**次级数据集**，该数据集依然使用初始数据集的标签；
  > - 根据新的数据集训练**次级学习器**；
  > - **多级学习器**的构建过程类似。



---

## QA

### 0. Boosting 与 Bagging 区别

- Bagging中每个训练集互不相关，也就是每个基分类器互不相关，而Boosting中训练集要在上一轮的结果上进行调整，也使得其不能并行计算
- Bagging中预测函数是均匀平等的，但在Boosting中预测函数是加权的

### 1. Boosting, Bagging 与偏差，方差

**Boosting** 能提升弱分类器性能的原因是降低了**偏差**；**Bagging** 则是降低了**方差**；

- Boosting：

  > - Boosting 的**基本思路**就是在不断减小模型的**训练误差**（拟合残差或者加大错类的权重），加强模型的学习能力，从而减小偏差；
  > - 但 Boosting 不会显著降低方差，因为其训练过程中各基学习器是强相关的，缺少独立性。

- Bagging：

  > - 对 `n` 个**独立不相关的模型**预测结果取平均，方差是原来的 `1/n`；
  > - 假设所有基分类器出错的概率是独立的，**超过半数**基分类器出错的概率会随着基分类器的数量增加而下降。
  
###   LightGBM 和 XGBoost 的结构差异
在过滤数据样例寻找分割值时，LightGBM 使用的是全新的技术：基于梯度的单边采样（GOSS）；而 XGBoost 则通过预分类算法和直方图算法来确定最优分割。这里的样例（instance）表示观测值/样本。

首先让我们理解预分类算法如何工作：
* 对于每个节点，遍历所有特征
* 对于每个特征，根据特征值分类样例
* 进行线性扫描，根据当前特征的基本信息增益，确定最优分割
* 选取所有特征分割结果中最好的一个

简单说，直方图算法在某个特征上将所有数据点划分到离散区域，并通过使用这些离散区域来确定直方图的分割值。虽然在计算速度上，和需要在预分类特征值上遍历所有可能的分割点的预分类算法相比，直方图算法的效率更高，但和 GOSS 算法相比，其速度仍然更慢。

### 为什么 GOSS 方法如此高效？

在 Adaboost 中，样本权重是展示样本重要性的很好的指标。但在梯度提升决策树（GBDT）中，并没有天然的样本权重，因此 Adaboost 所使用的采样方法在这里就不能直接使用了，这时我们就需要基于梯度的采样方法。

梯度表征损失函数切线的倾斜程度，所以自然推理到，如果在某些意义上数据点的梯度非常大，那么这些样本对于求解最优分割点而言就非常重要，因为算其损失更高。

GOSS 保留所有的大梯度样例，并在小梯度样例上采取随机抽样。比如，假如有 50 万行数据，其中 1 万行数据的梯度较大，那么我的算法就会选择（这 1 万行梯度很大的数据+x% 从剩余 49 万行中随机抽取的结果）。如果 x 取 10%，那么最后选取的结果就是通过确定分割值得到的，从 50 万行中抽取的 5.9 万行。

在这里有一个基本假设：如果训练集中的训练样例梯度很小，那么算法在这个训练集上的训练误差就会很小，因为训练已经完成了。

为了使用相同的数据分布，在计算信息增益时，GOSS 在小梯度数据样例上引入一个常数因子。因此，GOSS 在减少数据样例数量与保持已学习决策树的准确度之间取得了很好的平衡。

LightGBM一大的特点是在传统的GBDT基础上引入了两个 新技术和一个改进：

（1）Gradient-based One-Side Sampling(GOSS)技术是**去掉了很大一部分梯度很小的数据，只使用剩下的去估计信息增益，避免低梯度长尾部分的影响**。由于梯度大的数据在计算信息增益的时候更重要，所以GOSS在小很多的数据上仍然可以取得相当准确的估计值。

（2）Exclusive Feature Bundling(EFB)技术是指捆绑互斥的特征(i.e.，他们经常同时取值为0)，以减少特征的数量。但对互斥特征寻找最佳的捆绑方式是一个NP难问题，当时贪婪算法可以取得相当好的近似率(因此可以在不显著影响分裂点选择的准确性的情况下，显著地减少特征数量)。

（3）**在传统GBDT算法中，最耗时的步骤是找到最优划分点，传统方法是Pre-Sorted方式，其会在排好序的特征值上枚举所有可能的特征点**，而LightGBM中会使用histogram算法替换了传统的Pre-Sorted。基本思想是先把连续的浮点特征值离散化成k个整数，同时构造出图8所示的一个宽度为k的直方图。最开始时将离散化后的值作为索引在直方图中累积统计量，当遍历完一次数据后，直方图累积了离散化需要的统计量，之后进行节点分裂时，可以根据直方图上的离散值，从这k个桶中找到最佳的划分点，从而能更快的找到最优的分割点，而且因为直方图算法无需像Pre-Sorted那样存储预排序的结果，而只是保存特征离散过得数值，所以使用直方图的方式可以减少对内存的消耗。

Pre-sorted 算法需要 O(data) 次的计算
Histogram 算法只需要计算 O(bins) 次, 并且 bins 远少于data（直方图仍然需要 O(#data) 次来构建直方图, 而这仅仅包含总结操作，只是第一次做data此即可）

### 每个模型是如何处理属性分类变量的？£
#### CatBoost

CatBoost 可赋予分类变量指标，进而通过独热最大量得到独热编码形式的结果（独热最大量：在所有特征上，对小于等于某个给定参数值的不同的数使用独热编码）。

如果在 CatBoost 语句中没有设置「跳过」，CatBoost 就会将所有列当作数值变量处理。

注意，如果某一列数据中包含字符串值，CatBoost 算法就会抛出错误。另外，带有默认值的 int 型变量也会默认被当成数值数据处理。在 CatBoost 中，必须对变量进行声明，才可以让算法将其作为分类变量处理。

对于可取值的数量比独热最大量还要大的分类变量，CatBoost 使用了一个非常有效的编码方法，这种方法和均值编码类似，但可以降低过拟合情况。它的具体实现方法如下：

1. 将输入样本集随机排序，并生成多组随机排列的情况。

2. 将浮点型或属性值标记转化为整数。

3. 将所有的分类特征值结果都根据以下公式，转化为数值结果。