# 模型 - 回归树
[toc]

目标变量可以采用一组离散值的树模型称为分类树(常用的分类树算法有ID3、C4.5、CART)，而目标变量可以采用连续值（通常是实数）的决策树被称为回归树(如CART算法)。

## CART 用于回归
[参考网址](https://blog.csdn.net/weixin_36586536/article/details/80468426)

CART(classificationandregressiontree),CART(classificationandregressiontree), 分类回归树算法，既可用于分类也可用于回归，在这一部分我们先主要将其分类树的生成。区别于ID3ID3和C4.5C4.5,CARTCART假设决策树是二叉树，**内部节点特征的取值为“是”和“否”，左分支为取值为“是”的分支，右分支为取值为”否“的分支。这样的决策树等价于递归地二分每个特征**，将输入空间(即特征空间)划分为有限个单元。CARTCART的分类树用基尼指数来选择最优特征的最优划分点，具体过程如下

1. 从根节点开始，对节点计算现有特征的基尼指数，对每一个特征，例如A，再对其每个可能的取值如a,根据样本点对A=a的结果的”是“与”否“划分为两个部分，利用$$
\operatorname{Gini}(D, A=a)=\frac{\left|D_{1}\right|}{|D|} \operatorname{Gini}\left(D_{1}\right)+\frac{\left|D_{2}\right|}{|D|} \operatorname{Gini}\left(D_{2}\right)
$$
进行计算；
1. 在所有可能的特征A以及该特征所有的可能取值a中，选择**基尼指数最小的特征及其对应的取值作为最优特征和最优切分点**。然后根据最优特征和最优切分点，将本节点的数据集二分，生成两个子节点
2. 对两个字节点递归地调用上述步骤，直至**节点中的样本个数小于阈值，或者样本集的基尼指数小于阈值**，或者没有更多特征后停止；

3. 生成CART分类树；


## 回归树
回归树是可以用于回归的决策树模型，一个回归树对应着输入空间（即特征空间）的一个划分以及在划分单元上的输出值.与分类树不同的是，回归树对输入空间的划分采用一种启发式的方法，会遍历所有输入变量，找到最优的切分变量j和最优的切分点s，即选择第j个特征${x_j}$和它的取值s将输入空间划分为两部分，然后重复这个操作。

而如何找到最优的j和s是通过比较不同的划分的误差来得到的。一个输入空间的划分的误差是用真实值和划分区域的预测值的最小二乘来衡量的，即

$$
\sum_{x_{i} \in R_{m}}\left(y_{i}-f\left(x_{i}\right)\right)^{2}
$$

其中，$f(x_i)$是每个划分单元的预测值，这个预测值是该单元内每个样本点的值的均值，即
$$f(xi)=cm=ave(yi|xi∈Rm)$$

(将输入空间划分为MM个单元R1,R2,...,RmR1,R2,...,Rm)
那么，j和s的求解可以用下式进行

$$
\min _{j, s}\left[\min _{c_{1}} \sum_{x_{i} \in R_{1}(j, s)}\left(y_{i}-c_{1}\right)^{2}+\min _{c_{2}} \sum_{x_{i} \in R_{2}(j, s)}\left(y_{i}-c_{2}\right)^{2}\right]
$$

其中， $R1(j,s)=R1(j,s)= { x|xj≤sx|xj≤s}$和 $R2(j,s)=R2(j,s)= { x|xj>sx|xj>s}$是被划分后的两个区域

举个例子，我们要对南京市各地区的房价进行回归预测，我们将输入空间不断的按最小误差进行划分，得到类似下图的结果，将空间划分后，我们会用该单元内的均值作为该单元的预测值，如图中一片区域的平均房价作为该划分单元内房价的预测值(唔，怎么感觉这个例子还是不太准确…）。

![](https://img-blog.csdn.net/20180527114954501?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNjU4NjUzNg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
