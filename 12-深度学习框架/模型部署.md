
# 深度学习模型
[toc]
## 需求一：简单的demo演示，只要看看效果的

caffe、tf、pytorch等框架随便选一个，切到test模式，拿python跑一跑就好，顺手写个简单的GUI展示结果高级一点，可以用CPython包一层接口，然后用C++工程去调用

## 需求二：要放到服务器上去跑，但一不要求吞吐二不要求时延
caffe、tf、pytorch等框架随便选一个，按照官方的部署教程，老老实实用C++部署，例如pytorch模型用工具导到libtorch下跑（官方有教程，很简单）这种还是没有脱离框架，有很多为训练方便保留的特性没有去除，性能并不是最优的；另外，这些框架要么CPU，要么NVIDIA GPU，对硬件平台有要求，不灵活；还有，框架是真心大，占内存（tf还占显存），占磁盘

## 需求三：放到服务器上跑，要求吞吐和时延（重点是吞吐）
这种应用在互联网企业居多，一般是互联网产品的后端AI计算，例如人脸验证、语音服务、应用了深度学习的智能推荐等。由于一般是大规模部署，这时不仅仅要考虑吞吐和时延，还要考虑功耗和成本。所以除了软件外，硬件也会下功夫，比如使用推理专用的NVIDIA P4、寒武纪MLU100等。这些推理卡比桌面级显卡功耗低，单位能耗下计算效率更高，且硬件结构更适合高吞吐量的情况软件上，一般都不会直接上深度学习框架。对于NVIDIA的产品，一般都会使用TensorRT来加速（我记得NVIDIA好像还有TensorRT inference server什么的，名字记不清了，反正是不仅可以加速前传，还顺手帮忙调度了）。TensorRT用了CUDA、CUDNN，而且还有图优化、fp16、int8量化等。反正用NVIDIA的一套硬软件就对了

## 需求四：放在NVIDIA嵌入式平台上跑，注重时延比如PX2、TX2、Xavier等，
参考上面（用全家桶就对了），也就是贵一点嘛

## 需求五：放在其他嵌入式平台上跑，注重时延硬件方面，要根据模型计算量和时延要求，
结合成本和功耗要求，选合适的嵌入式平台。比如模型计算量大的，可能就要选择带GPU的SoC，用opencl/opengl/vulkan编程；也可以试试NPU，不过现在NPU支持的算子不多，一些自定义Op多的网络可能部署不上去对于小模型，或者帧率要求不高的，可能用CPU就够了，不过一般需要做点优化（剪枝、量化、SIMD、汇编、Winograd等）顺带一提，在手机上部署深度学习模型也可以归在此列，只不过硬件没得选，用户用什么手机你就得部署在什么手机上23333。为老旧手机部署才是最为头疼的上述部署和优化的软件工作，在一些移动端开源框架都有人做掉了，一般拿来改改就可以用了，性能都不错。

## 需求六：上述部署方案不满足我的需求比如开源移动端框架速度不够——自己写一套。
比如像商汤、旷世、Momenta都有自己的前传框架，性能应该都比开源框架好。只不过自己写一套比较费时费力，且如果没有经验的话，很有可能费半天劲写不好

## 实际部署
如果你使用的神经网络框架是TensorFlow，那么TensorFlow Serving是你非常好的选择。目前本人用的是TensorFlow Serving + Docker + Tornado的组合，Docker非常易于部署任何模型，而Tornado负责处理高并发请求。

另外，如果你使用的是其它神经网络框架，例如caffe、pytorch，我会推荐Nvidia的TensorRT Inference Server，它支持所有模型的部署，包括TF系、ONNX系、mxnet等等，TRT会先对你的网络进行融合，合并可以同步计算的层，然后量化计算子图，让你的模型以float16、int8等精度进行推理，大大加速推理速度，而你只需要增加几行简单的代码就能实现。而且TRT Inference Server能够处理负载均衡，让你的GPU保持高利用率。