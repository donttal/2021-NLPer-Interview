# RNN 

tags: 深度学习

---

[TOC]

## 基础相关

- [RNN ： NLP中最常见的神经网络单元](https://zhuanlan.zhihu.com/p/44106750)
- [LSTM：RNN最常用的变体](https://zhuanlan.zhihu.com/p/44124492)
- [RNN 的梯度消失问题](https://zhuanlan.zhihu.com/p/44163528)
- [RNN vs LSTM vs GRU -- 该选哪个？](https://zhuanlan.zhihu.com/p/55386469)

## QA

### 1. RNN 中为何会出现梯度消失，梯度爆炸问题

[RNN 的梯度消失问题](https://zhuanlan.zhihu.com/p/44163528)
![](![](https://cdn.jsdelivr.net/gh/donttal/figurebed/img/20210306114911.png))

无论是梯度消失还是梯度爆炸，都是源于网络结构太深，造成网络权重不稳定，从本质上来讲是因为梯度反向传播中的连乘效应。

我们注意到， 首先三个门的激活函数是sigmoid， 这也就意味着这三个门的输出要么接近于0 ， 要么接近于1。这就使得
$$
\frac{\delta c_{t}}{\delta c_{t-1}}=f_{t}, \quad \frac{\delta h_{t}}{\delta h_{t-1}}=o_{t}
$$
是非0即1的，当门为1时， 梯度能够很好的在LSTM中传递，很大程度上减轻了梯度消失发生的概率， 当门为0时，说明上一时刻的信息对当前时刻没有影响， 我们也就没有必要传递梯度回去来更新参数了。所以， 这就是为什么通过门机制就能够解决梯度的原因： 使得单元间的传递 [公式] 为0 或 1。

$$
\frac{\delta S_{j}}{\delta S_{j-1}}
$$


### 2. Relu 能否作为RNN的激活函数

答案是可以，但会产生一些问题：

> - 换成 Relu 可能使得输出值变得特别大，从而产生溢出
> - 换成Relu 也不能解决梯度消失，梯度爆炸问题，因为还有 $W_s$ 连乘的存在（如1中公式）

为什么 CNN 和前馈神经网络采用Relu 就能解决梯度消失，梯度爆炸问题？

> 因为CNN 或 FNN 中各层的 W 并不相同， 且初始化时是独立同分布的，一定程度熵可以抵消。
>
> 而 RNN 中各层矩阵 $W_s$ 是一样的。

### 3. 推导 LSTM

关键在于三个门， 三个状态。 其中三个门的公式基本一样

- 遗忘门： 决定上一时刻的 $c_{t-1}$ 多少保留到当前时刻 $c_t$：

$$
f_t = \sigma{(W_f \cdot [h_{t-1},x_t] + b_f)} 
$$

- 输入门：决定当前时刻输入 $x_t$ 有多少保存到当前时刻 $c_t$：
  $$
   i_t=\sigma(W_i\cdot[h_{t-1},x_t]+b_i) 
  $$

- 输出门：控制当前时刻的 $c_t$ 有多少信息作为当前时刻的 $h_t$:
  $$
   o_t=\sigma(W_o\cdot[h_{t-1},x_t]+b_o) 
  $$

- 当前输入的状态 $\tilde{c_t}$： 将上一时刻的 $h_{t-1}$ 与当前时刻的 $x_t$ 融合：
  $$
  \tilde{c}_t=\tanh(W_c\cdot[h_{t-1},x_t]+b_c)
  $$

- 当前时刻的状态 $c_t$： 将 $x_t$ ， $\tilde{c_t}$ , $h_{t-1}$ 融合：
  $$
   c_t=f_t \circ c_{t-1}+i_t \circ \tilde{c}_t 
  $$

- 当前时刻的输出 $h_t$：从 $c_t$ 中分出一部分信息：
  $$
  h_t=o_t\circ \tanh(c_t)
  $$




### 4. LSTM 长短记忆机制

长短记忆机制主要通过 **输入门** 与 **遗忘门** 来实现：

- 如果当前信息$x_t$不重要， 则**输入门**相应维度接近于 0， 当前的信息就几乎不融入进入 $\tilde{c_t}$ 。反之， 输入门相应维度接近于 1， 当前信息实现很好的融入。
- 如果之前的信息 $c_{t-1}$ 不重要，则遗忘门相应维度接近于 0， 过去的信息就几乎不融入进  $c_t$。 反之，亦然。

### 5. LSTM的门机制为何选择 sigmoid 作为激活函数？

值得一提的是， 目前几乎所有主流的门控机制中，门控单元的选择均使用 sigmoid 。

- sigmoid 的饱和性： 十分符合 门控 的效果
- 值域在 (0,1)， 当输入较大或较小时，输出会接近1 或 0， 从而保证门的开或关。

### 6.  融合信息时为何选择 tanh？

- 值域为 (-1, 1)， 这样会带来两个好处：

  > - 与大多数情景下特征分布以 0 为中心相吻合。（激活函数一章中有提到这点特性的重要性）
  > - 可以避免前向传播时的数值溢出问题(主要是上溢)

- tanh 在 0 附近有较大的梯度，模型收敛更快

### 7. 计算资源有限的情况下有没有什么优化方法？

- 采用 Hard gate
- 采用 GRU

### 8. 推导一下 GRU 

两个门，两个状态

- 更新门：控制前一时刻状态信息与当前输入融合
  $$
  z_t = \sigma (W_z x_t + U_z h_{t-1}) 
  $$

- 重置门：控制前一时刻状态信息
  $$
  r_t = \sigma(W_r x_t + U_r h_{t-1})
  $$

- 当前输入的信息融入 $h_t'$： 
  $$
  h_t' = tanh(Wx_t + r_t \odot Uh_{t-1}) \\
  $$

- 当前时刻的状态：
  $$
  h_t = z_t \odot h_{t-1} + (1-z_t) \odot h_{t-1}'
  $$




### 9. LSTM 与 GRU 之间的关系

- GRU 认为**遗忘门**与**输入门**功能有一定的的重合，认为之前的信息 与 当前的输入信息是此消彼长的关系，因此将二者合并成一个门： 更新门。
- 合并了记忆状态 c 与隐藏状态 h
- 采用**重置门**代替了**输出门**

### 10 为何RNN 会有梯度消失现象，推一下？

[RNN 的梯度消失问题](https://zhuanlan.zhihu.com/p/44163528)

### 11. LSTM 与 GRU 区别

- LSTM 中的单元状态 c 与 GRU 中的 h 类似，但 GRU 去掉了 h 这个状态，即最后的输出不再进行调节，那么也就不需要输出门了
- 在产生新的全局状态时， LSTM 采用 输入门+遗忘门 的方式， 而 GRU 只采用更新门来控制
- 更新门起到了遗忘门的作用， 重置门起到了输入门的作用。

### 12. 为何 RNN 训练时 loss 波动很大

由于RNN特有的memory会影响后期其他的RNN的特点，梯度时大时小，learning rate没法个性化的调整，导致RNN在train的过程中，Loss会震荡起伏。

为了解决RNN的这个问题，在训练的时候，可以设置临界值，当梯度大于某个临界值，直接截断，用这个临界值作为梯度的大小，防止大幅震荡。

### 13. LSTM 中的激活函数选择

LSTM， 门的激活函数选择 Sigmoid， 而在生成 c 时采用 tanh。

- Sigmoid函数的输出在0～1之间，符合门控的物理定义。且当输入较大或较小时，其输出会非常接近1或0，从而保证该门开或关。
- 在生成候选记忆时，使用Tanh函数，是因为其输出在−1～1之间，这与大多数场景下特征分布是0中心的吻合。此外，Tanh函数在输入为0附近相比Sigmoid函数有更大的梯度，通常使模型收敛更快。