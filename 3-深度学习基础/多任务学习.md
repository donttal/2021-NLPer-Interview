# 多任务学习概述
[toc]

[参考网址](https://zhuanlan.zhihu.com/p/59413549)

背景：只专注于单个模型可能会忽略一些相关任务中可能提升目标任务的潜在信息，通过进行一定程度的共享不同任务之间的参数，可能会使原任务泛化更好。广义的讲，只要loss有多个就算MTL，一些别名（joint learning，learning to learn，learning with auxiliary task）



目标：通过权衡主任务与辅助的相关任务中的训练信息来提升模型的泛化性与表现。从机器学习的视角来看，MTL可以看作一种inductive transfer（先验知识），通过提供inductive bias（某种对模型的先验假设）来提升模型效果。比如，使用L1正则，我们对模型的假设模型偏向于sparse solution（参数要少）。在MTL中，这种先验是通过auxiliary task来提供，更灵活，告诉模型偏向一些其他任务，最终导致模型会泛化得更好。



## MTL Methods for DNN

### hard parameter sharing (此方法已经有26岁了<1993>）
在所有任务中共享一些参数（一般底层），在特定任务层（顶层）使用自己独有参数。这种情况，共享参数得过拟合几率比较低（相对非共享参数），过拟合的几率是O(#tasks). [1]
![](https://pic3.zhimg.com/80/v2-682854e86f8013c4a10891afbadcfb22_1440w.jpg)

### soft parameter sharing
每个任务有自己的参数，最后通过对不同任务的参数之间的差异加约束，表达相似性。比如可以使用L2, trace norm等。
![](https://pic3.zhimg.com/v2-6420b435d7f17a40281a025d023e6212_r.jpg)

优点及使用场景

* implicit data augmentation: 每个任务多少都有样本噪声，不同的任务可能噪声不同，最终多个任务学习会抵消一部分噪声（类似**bagging**的思想，不同任务噪声存在于各个方向，最终平均就会趋于零）
* 一些噪声很大的任务，或者训练样本不足，维度高，模型可能无法有效学习，甚至无法无法学习到相关特征
* 某些特征可能在主任务不好学习（比如只存在很高阶的相关性，或被其他因素抑制），但在辅助任务上好学习。可以通过辅助任务来学习这些特征，方法比如hints（预测重要特征）[2]
* 通过学习足够大的假设空间，在未来某些新任务中可以有较好的表现（解决冷启动），前提是这些任务都是同源的。
* 作为一种正则方式，约束模型。所谓的inductive bias。缓解过拟合，降低模型的Rademacher complexity（拟合噪声的能力，用于衡量模型的能力）


## DNN中的MTL

### Deep Relation Network [20]

计算机视觉中，一般共享卷积层，之后是任务特定的DNN层。通过对任务层设定先验，使模型学习任务之间的关系。
![](https://pic1.zhimg.com/v2-264722d5cb4076c9a590cf74ddad0b18_r.jpg)

### Fully-Adaptive Feature Sharing [21]

从一个简单结构开始，贪心地动态地加宽模型，使相似的模型聚簇。贪心方法可能无法学到全局最优结构；每个分支一个任务无法学习任务之间的复杂关系。


### Cross-stitch Networks [22]

soft parameter sharing，通过线性组合学习前一层的输出，允许模型决定不同任务之间的分享程度
![](https://pic3.zhimg.com/v2-badfd69079bb1fd93e939209c184412a_r.jpg)

### Low supervision [23]

寻找更好的多任务结构，复杂任务的底层应该被低级任务目标来监督（比如NLP前几层学习一个NER或POS辅助任务）

### A Joint Many-task Model [24]

对多个NLP任务预先设定层级结构，之后joint learning
![](https://pic1.zhimg.com/v2-f12d01d4d9569cdb2d0aeca5b8e4d988_r.jpg)

### Sluice Networks [27]

大杂烩（hard parameter sharing + cross stitch networks + block-sparse regularization + task hierarchy(NLP) )，使得模型自己学习哪些层，哪些子空间来共享，在哪层模型找到了inputs的最优表达。

![](https://pic3.zhimg.com/v2-e089f78832339ac4e96efb9611e5cf4e_r.jpg)

当不同的任务相关性大，近似服从相同的分布，共享参数是有益的，如果相关性不大或者不相关的任务呢？

早期工作是预先为每对任务指定哪些层来分享，这种方法扩展性差且模型结构严重有偏；当**任务相关性下降或需要不同level推理**时，hard parameter sharing就不行了。

目前比较火的是learning what to share（outperform hard parameter sharing）；还有就是对任务层级进行学习在任务具有多粒度因素时也是有用的。