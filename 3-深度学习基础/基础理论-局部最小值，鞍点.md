# 局部最小值，鞍点
[toc]

---

## 基础原理

[神经网络最终收敛何处？](<https://zhuanlan.zhihu.com/p/48737640>)

## 局部最小值
很容易想到，收敛过程中遇到的第一个阻碍就是“局部最小值点”。

从数学的角度来说，局部最小值点首先满足一阶导数均为0， 我们还要保证曲线是凹的，也就是说，局部最小值点二阶导数均大于0。 总结来说，局部最小值满足两个条件：

对于所有参数的一阶导数均为0.
对于所有参数的二阶参数均大于0.

## 鞍点
损失函数中遇到的第二个阻碍便是“鞍点”。

从数学角度来说，鞍点同样满足两个条件：

对于所有参数的一阶导数均为0.
存在对于某一个参数，其二阶导数小于0.

![](https://pic1.zhimg.com/v2-04ff4df173b9516a00d82ae4a56d2c04_r.jpg)

## QA

---

### 1. 如何避免陷入局部最小值与鞍点？

- SGD 或 Mini-batch：SGD 与 Mini-batch 引入了随机性，每次以部分样本来计算梯度，能够相当程度上避免陷入局部最小值。
- 动量： 引入动量，相当于引入惯性。一些常见情况时，如上次梯度过大，导致进入局部最小点时，下一次更新能很容易借助上次的大梯度跳出局部最小点。
- 自适应学习率：通过学习率来控制梯度是一个很棒的思想， 自适应学习率算法能够基于历史的累计梯度去计算一个当前较优的学习率。

