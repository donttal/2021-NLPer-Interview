# 深度学习网络模型压缩和加速
[toc]


[参考网址](https://zhuanlan.zhihu.com/p/36051603)
本文全面概述了深度神经网络的压缩方法，主要可分为参数修剪与共享、低秩分解、迁移/压缩卷积滤波器和知识精炼，论文对每一类方法的性能、相关应用、优势和缺陷等方面进行了独到分析。
论文 | A Survey of Model Compression and Acceleration for Deep Neural Networks

## 模型压缩方法
综合现有的深度模型压缩方法，它们主要分为四类：

* 参数修剪和共享（parameter pruning and sharing）
* 低秩因子分解（low-rank factorization）
* 转移/紧凑卷积滤波器（transferred/compact convolutional filters）
* 知识蒸馏（knowledge distillation）

基于参数修剪和共享的方法针对模型参数的冗余性，试图去除冗余和不重要的项。基于低秩因子分解的技术使用矩阵/张量分解来估计深度学习模型的信息参数。基于传输/紧凑卷积滤波器的方法设计了特殊的结构卷积滤波器来降低存储和计算复杂度。知识蒸馏方法通过学习一个蒸馏模型，训练一个更紧凑的神经网络来重现一个更大的网络的输出。

### 优缺点
一般来说，参数修剪和共享，低秩分解和知识蒸馏方法可以用于全连接层和卷积层的 CNN，但另一方面，使用转移/紧凑型卷积核的方法仅支持卷积层。

低秩因子分解和基于转换/紧凑型卷积核的方法提供了一个端到端的流水线，可以很容易地在 CPU/GPU 环境中实现。相反参数修剪和共享使用不同的方法，如矢量量化，二进制编码和稀疏约束来执行任务，这导致常需要几个步骤才能达到目标。

### 参数修剪和共享

根据减少冗余（信息冗余或参数空间冗余）的方式，这些参数修剪和共享可以进一步分为三类：**模型量化和二进制化、参数共享和结构化矩阵**（structural matrix）。


网络量化通过减少表示每个权重所需的比特数来压缩原始网络。Gong et al. 对参数值使用 K-Means 量化。Vanhoucke et al. 使用了 **8 比特参数量化**可以在准确率损失极小的同时实现大幅加速。

Han S 提出一套完整的深度网络的压缩流程：
* 首先修剪不重要的连接，重新训练稀疏连接的网络。
* 然后使用权重共享量化连接的权重，
* 再对量化后的权重和码本进行霍夫曼编码，以进一步降低压缩率。


在量化级较多的情况下准确率能够较好保持，**但对于二值量化网络的准确率在处理大型 CNN 网络，如 GoogleNet 时会大大降低。**另一个缺陷是现有的二进制化方法都基于简单的矩阵近似，忽视了二进制化对准确率损失的影响。

### 剪枝和共享
网络剪枝和共享起初是解决过拟合问题的，现在更多得被用于降低网络复杂度。

早期所应用的剪枝方法称为偏差权重衰减（Biased Weight Decay），其中最优脑损伤（Optimal Brain Damage）和最优脑手术（Optimal Brain Surgeon）方法，是基于损失函数的 Hessian 矩阵来减少连接的数量。他们的研究表明这种剪枝方法的精确度比基于重要性的剪枝方法（比如 Weight Decay 方法）更高。这个方向最近的一个趋势是在预先训练的 CNN 模型中修剪冗余的、非信息量的权重。

在稀疏性限制的情况下培训紧凑的 CNN 也越来越流行，这些稀疏约束通常作为 l_0 或 l_1 范数调节器在优化问题中引入。

剪枝和共享方法存在一些潜在的问题。首先，若使用了 l_0 或 l_1 正则化，则剪枝方法需要**更多的迭代次数才能收敛**，此外，所有的剪枝方法都需要**手动设置层的超参数**，在某些应用中会显得很复杂。

低秩方法很适合模型压缩和加速，但是低秩方法的实现并不容易，因为它涉及计算成本高昂的分解操作。另一个问题是**目前的方法都是逐层执行低秩近似**，无法执行全局参数压缩，因为不同的层具备不同的信息。最后，分解需要大量的重新训练来达到收敛。

### 迁移/压缩卷积滤波器
使用紧凑的卷积滤波器可以直接降低计算成本。在 Inception 结构中使用了将 3×3 卷积分解成两个 1×1 的卷积；SqueezeNet 提出用 1×1 卷积来代替 3×3 卷积，与 AlexNet 相比，SqueezeNet 创建了一个紧凑的神经网络，参数少了 50 倍，准确度相当。

这种方法仍有一些小问题解决。首先，这些方法擅长处理广泛/平坦的体系结构（如 VGGNet）网络，而不是狭窄的/特殊的（如 GoogleNet，ResidualNet）。其次，转移的假设有时过于强大，不足以指导算法，导致某些数据集的结果不稳定。

### 知识蒸馏

利用知识转移（knowledge transfer）来压缩模型最早是由 Caruana 等人提出的。他们训练了带有伪数据标记的强分类器的压缩/集成模型，并复制了原始大型网络的输出，但是，这项工作仅限于浅模型。

后来改进为知识蒸馏，将深度和宽度的网络压缩成较浅的网络，其中压缩模型模拟复杂模型所学习的功能，主要思想是通过学习通过 softmax 获得的类分布输出，将知识从一个大的模型转移到一个小的模型。

Hinton 的工作引入了知识蒸馏压缩框架，即通过遵循“学生-教师”的范式减少深度网络的训练量，这种“学生-教师”的范式，即通过软化“教师”的输出而惩罚“学生”。为了完成这一点，学生学要训练以预测教师的输出，即真实的分类标签。这种方法十分简单，但它同样在各种图像分类任务中表现出较好的结果。

基于知识蒸馏的方法能令更深的模型变得更加浅而显著地降低计算成本。但是也有一些缺点，例如只能用于具有 Softmax 损失函数分类任务，这阻碍了其应用。另一个缺点是模型的假设有时太严格，其性能有时比不上其它方法。

## 讨论与挑战
深度模型的压缩和加速技术还处在早期阶段，目前还存在以下挑战：

依赖于原模型，降低了修改网络配置的空间，对于复杂的任务，尚不可靠；
通过减少神经元之间连接或通道数量的方法进行剪枝，在压缩加速中较为有效。但这样会对下一层的输入造成严重的影响；
结构化矩阵和迁移卷积滤波器方法必须使模型具有较强的人类先验知识，这对模型的性能和稳定性有显著的影响。研究如何控制强加先验知识的影响是很重要的；
知识精炼方法有很多优势，比如不需要特定的硬件或实现就能直接加速模型。个人觉得这和迁移学习有些关联。
多种小型平台（例如移动设备、机器人、自动驾驶汽车）的硬件限制仍然是阻碍深层 CNN 发展的主要问题。相比于压缩，可能模型加速要更为重要，专用芯片的出现固然有效，但从数学计算上将乘加法转为逻辑和位移运算也是一种很好的思路。




