# 常用6个激活函数[toc]

---

[参考网址](https://mp.weixin.3233795485.com/s/_XsM1_QlOAOY11v9Y_b4fg)


## sigmoid激活函数
![](https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9PrS2jqcgp04sYOZNhbMVWe5nFPYqgmwEMyFYMqhWsHUjkwrJLPpeTvVRTGOF54Q7sgCInu1ME0w/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

Sigmoid 函数的图像看起来像一个 S 形曲线。

函数表达式如下：

![](https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9PrS2jqcgp04sYOZNhbMVW9xApDdYWhQEq9LLAiaCywPBnduQKcpDmoLM9hu3FKziaurBUz0M9vTicg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

$$
f^{\prime}(z)=f(z)(1-f(z))
$$

**在什么情况下适合使用 Sigmoid 激活函数呢？**

* Sigmoid 函数的输出范围是 0 到 1。由于输出值限定在 0 到 1，因此它对每个神经元的输出进行了归一化；

* 用于将预测概率作为输出的模型。由于概率的取值范围是 0 到 1，因此 Sigmoid 函数非常合适；

* 梯度平滑，避免「跳跃」的输出值；

* 函数是可微的。这意味着可以找到任意两个点的 sigmoid 曲线的斜率；

* 明确的预测，即非常接近 1 或 0。


**Sigmoid 激活函数有哪些缺点？**

* 倾向于梯度消失；

* 函数输出不是以 0 为中心的，这会降低权重更新的效率；

* Sigmoid 函数执行指数运算，计算机运行得较慢。



## tanh激活函数
tanh 激活函数的图像也是 S 形，表达式如下：

![](https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9PrS2jqcgp04sYOZNhbMVWIbzla17UricR6icgRpD6c9hBG7s1kbeTz2beiaJTLEa5xicBPaRuMtVNyA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)


tanh 是一个双曲正切函数。tanh 函数和 sigmoid 函数的曲线相对相似。但是它比 sigmoid 函数更有一些优势。

![](https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9PrS2jqcgp04sYOZNhbMVWtz9Dn7SzuKsicEnDnGEegkH3Wlt5FE2ybkyXdW6m363azzMA0ibbraPA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)


* 首先，当输入较大或较小时，输出几乎是平滑的并且梯度较小，这不利于权重更新。二者的区别在于输出间隔，tanh 的输出间隔为 1，并且整个函数以 0 为中心，比 sigmoid 函数更好；

* 在 tanh 图中，负输入将被强映射为负，而零输入被映射为接近零。


注意：在一般的二元分类问题中，tanh 函数用于隐藏层，而 sigmoid 函数用于输出层，但这并不是固定的，需要根据特定问题进行调整。

## ReLU激活函数
![](https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9PrS2jqcgp04sYOZNhbMVWPM1mXg0v5gS5icaealpwTJPm2xQ4YOYlqsPotOcgkpIEBuLzk6bauMA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

ReLU 激活函数图像如上图所示，函数表达式如下：

![](https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9PrS2jqcgp04sYOZNhbMVWaqo9DlyxicY3SBzYPlhbboraoWt9icAAHlgudm8DaXhRzzFEoQmvo3sA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

ReLU 函数是深度学习中较为流行的一种激活函数，相比于 sigmoid 函数和 tanh 函数，它具有如下优点：

* 当输入为正时，不存在梯度饱和问题。

* 计算速度快得多。ReLU 函数中只存在线性关系，因此它的计算速度比 sigmoid 和 tanh 更快。


当然，它也有缺点：

* Dead ReLU 问题。当输入为负时，ReLU 完全失效，在正向传播过程中，这不是问题。有些区域很敏感，有些则不敏感。但是在反向传播过程中，**如果输入负数，则梯度将完全为零**，sigmoid 函数和 tanh 函数也具有相同的问题；

* 我们发现 ReLU 函数的输出为 0 或正数，这意味着 ReLU 函数不是以 0 为中心的函数。

## Leaky ReLU激活函数
![](https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9PrS2jqcgp04sYOZNhbMVWaqo9DlyxicY3SBzYPlhbboraoWt9icAAHlgudm8DaXhRzzFEoQmvo3sA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

为什么 Leaky ReLU 比 ReLU 更好？
![](https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9PrS2jqcgp04sYOZNhbMVWzaAABmeNFTftYWibibdX6icdlIl4bf8Wy2Gbo8lciaKra9hpNdhko4iaM1g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

* Leaky ReLU 通过把 x 的非常小的线性分量给予负输入（0.01x）来调整负值的零梯度（zero gradients）问题；

* leak 有助于扩大 ReLU 函数的范围，通常 a 的值为 0.01 左右；

* Leaky ReLU 的函数范围是（负无穷到正无穷）。

## ELU激活函数
![](https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9PrS2jqcgp04sYOZNhbMVWVfia1pv25cviaB4rdEoREh5AQIGND5JwflQQ11YKUovC6moiaSqicTwOoQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

ELU 的提出也解决了 ReLU 的问题。与 ReLU 相比，ELU 有负值，这会使激活的平均值接近零。均值激活接近于零可以使学习更快，因为它们使梯度更接近自然梯度。

![](https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9PrS2jqcgp04sYOZNhbMVWfguia0LXZjVRfoFS3PuViapJt9gygKnJVgfHDROzB0a8kcO8xQURyAcw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

显然，ELU 具有 ReLU 的所有优点，并且：

* 没有 Dead ReLU 问题，输出的平均值接近 0，以 0 为中心；

* ELU 通过减少偏置偏移的影响，使正常梯度更接近于单位自然梯度，从而使均值向零加速学习；

* ELU 在较小的输入下会饱和至负值，从而减少前向传播的变异和信息。


一个小问题是它的计算强度更高。与 Leaky ReLU 类似，尽管理论上比 ReLU 要好，但目前在实践中没有充分的证据表明 ELU 总是比 ReLU 好。

## Softmax函数
![](https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9PrS2jqcgp04sYOZNhbMVWmk9OHeNrtt74bsmaDV8l2kXic1Xlxxcv1LvFwuQILPKfm1e3jtDsibNw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

Softmax 是用于多类分类问题的激活函数，在多类分类问题中，超过两个类标签则需要类成员关系。对于长度为 K 的任意实向量，Softmax 可以将其压缩为长度为 K，值在（0，1）范围内，并且向量中元素的总和为 1 的实向量。

![](https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9PrS2jqcgp04sYOZNhbMVW0g4cnpBOe6ZCM8UcMw9yjHQ8cpIootGG1s4pibdKfeE4JFzYQiaNhdxg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

Softmax 与正常的 max 函数不同：max 函数仅输出最大值，但 Softmax 确保较小的值具有较小的概率，并且不会直接丢弃。我们可以认为它是 argmax 函数的概率版本或「soft」版本。

Softmax 函数的分母结合了原始输出值的所有因子，这意味着 Softmax 函数获得的各种概率彼此相关。

Softmax 激活函数的主要缺点是：

* 在零点不可微；

* 负输入的梯度为零，这意味着对于该区域的激活，权重不会在反向传播期间更新，因此会产生永不激活的死亡神经元。

## QA



## 0. Dead Relu 问题

某些神经元可能永远不会被激活， 导致其相应的参数永远不能被更新。其本质是**由于Relu在的小于0时其梯度为0所导致的。**

首先我们假设Relu的输入是一个低方差中心在+0.1的正态分布， 此时假设现在大多数Relu的输入是正数，那么大多数输入经过Relu函数能得到一个正值， 因此此时大多数输入能够反向传播通过Relu得到一个梯度， 于是我们的Relu的输入就完成了更新。

假设在随机反向传播中， 有一个巨大的梯度经过了Relu且此时Relu的输入为正（Relu是打开的）， 那么该梯度会引起Relu输入X的巨大变化， 假设此时输入X的分布变成了一个中心在-0.1 的正态分布。此时的情况如下：

> 首先， 大多数Relu的输入变为负数， 输入经过Relu函数就能得到一个0， 这也意味着大多数输入不能反向传播通过Relu得到一个梯度，导致这部分输入无法通过更新。

### 1. Relu VS Sigmoid VS tanh

- sigmoid 缺陷：

  > - 极容易导致梯度消失问题
  > - 计算费时
  > - **sigmoid 函数不是关于原点中心对称的**

- tanh 缺陷： 无法解决梯度消失问题

- Relu 优点：

  > - **一定程度上缓解了梯度问题：** 其导数始终为一个常数
  > - **计算速度非常快：** 求导不涉及浮点运算，所以速度更快
  > - **减缓过拟合：** `ReLU` 在负半区的输出为 0。一旦神经元的激活值进入负半区，那么该激活值就不会产生梯度/不会被训练，造成了网络的稀疏性——**稀疏激活**， 这有助于减少参数的相互依赖，缓解过拟合问题的发生

### 2. 为什么Relu 不是全程可微也能用于基于梯度的学习？

虽然 ReLU 在 0 点不可导，但是它依然存在**左导数和右导数**，只是它们不相等（相等的话就可导了），于是在实现时通常会返回左导数或右导数的其中一个，而不是报告一个导数不存在的错误。

### 3. 为何加入非线性因素能够加强网络的表示能力？

- 神经网络的万能近似定理：神经网络只要具有至少一个非线性隐藏层，那么只要给予网络足够数量的隐藏单元，它就可以以任意的精度来近似任何**从一个有限维空间到另一个有限维空间**的函数。
- 如果不使用非线性激活函数，那么每一层输出都是上层输入的**线性组合**；此时无论网络有多少层，其整体也将是线性的，这会导致失去万能近似的性质
- 但仅**部分层是纯线性**是可以接受的，这有助于**减少网络中的参数**。

### 4. 为何 tanh 比 sigmoid 收敛快？

$$
tanh^{'}(x)=1-tanh(x)^{2}\in (0,1) \\
sigmoid^{'}(x)=sigmoid(x)*(1-sigmoid(x))\in (0,\frac{1}{4}]
$$

### 5. 为何一般选择 softmax 为多分类的输出层

虽然能够将输出范围概率限制在 [0,1]之间的方法有很多，但 Softmax 的好处在于， 它使得输出两极化：正样本的结果趋近于 1， 负样本的结果趋近于 0。 可以说， Softmax 是 logistic 的一种泛化。




