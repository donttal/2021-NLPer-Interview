# ALBert论文阅读笔记
下表是bert的base版本12层，参数量接近110M在linux环境下基于GTX 1080运行结果。
![](https://pic1.zhimg.com/80/v2-4955a44d1889239ee7630a32abac7270_hd.jpg)
- 我们可以看到序列长度越短，推断时间也越短；
- GPU的推断性能远大于cpu；
- 基于tfserving的推断，因为是基于grpc远程调用，在服务启动和调用上会带来一定的耗时。 那么有没有一种压缩版的bert，可以提高线下训练和线上推断的性能呢？下面这篇论文或许可以提供一种解决方案。

## 1. 摘要
Bert在2018一经提出，提高了很多nlp任务的baseline，但是Bert模型参数量大，在推断资源有限的情况下， 我们应该怎么样用bert这种好的预训练模型。为了解决问题，本文提出了两种参数 简化的方法，加速Bert的预训练和推断；并且我们提出了一个新的自监督的loss函数（SOP）学习到句子间的内部特征。 我们提出的模型，参数量更小，并且在GLUE,RACE 等NLP 任务上达到最佳性能。

## ALBERT论文细节以及实验结果
### 参数缩减方法
本文提出两种模型参数缩减的方法，具体如下：

从模型角度来讲，wordPiece embedding是学习上下文独立的表征维度为E，而隐藏层embedding是 学习上下文相关的表征维度为H。为了应用的方便，原始的bert的向量维度E=H，这样一旦增加了H，E也就增大了。 ALBert提出向量参数分解法，将一个非常大的词汇向量矩阵分解为两个小矩阵，例如词汇量大小是V，向量维度是E，隐藏层向量为H，则 原始词汇向量参数大小为V*H，ALBert想将原始embedding映射到V*E（低纬度的向量），然后映射到隐藏空间H，这样参数量从 V*H下降到 V*E+E*H，参数量大大下降。但是要注意这样做的损失确保矩阵分解后的两个小矩阵的乘积损失，是一个有损的操作。

层之间参数共享。base的bert总共由12层的transformer的encoder部分组成，层参数共享方法避免了随着深度的加深带来的参数量的增大。 具体的共享参数有这几种，attention参数共享、ffn残差网络参数共享。

### SOP预训练任务
我们知道原始的Bert预训练的loss由两个任务组成，maskLM和NSP(Next Sentence Prediction)，maskLM通过预测mask掉的词语来实现真正的双向transformer， NSP类似于语义匹配的任务，预测句子A和句子B是否匹配，是一个二分类的任务，其中正样本从原始语料获得，负样本随机负采样。NSP任务可以 提高下游任务的性能，比如句子对的关系预测。但是也有论文指出NSP任务其实可以去掉，反而可以提高性能，比如RoBert。

本文以为NSP任务相对于MLM任务太简单了，学习到的东西也有限，因此本文提出了一个新的loss，sentence-order prediction(SOP)， SOP关注于句子间的连贯性，而非句子间的匹配性。SOP正样本也是从原始语料中获得，负样本是原始语料的句子A和句子B交换顺序。 举个例子说明NSP和SOP的区别，原始语料句子 A和B， NSP任务正样本是 AB，负样本是AC；SOP任务正样本是AB，负样本是BA。 可以看出SOP任务更加难，学习到的东西更多了（句子内部排序）。

### 实验结果
本文提出了2种参数压缩的方法以及1个新的loss，下面主要对这几种方法进行实验，实验结果如下：

下图2展示了BERT-large和ALBERT-large两个模型的输入和输出embedding的L2以及余弦距离，我们可以看出ALBERT模型的距离比BERT模型 更层距离更加平滑，这说明权重共享有助于稳定网络的参数。
![](https://pic4.zhimg.com/80/v2-5178eb5047d5ddd2372af238f60a96b7_hd.jpg)
其中BERT和ALBERT模型的参数配置如下，我们可以看到当减小embedding的维度以及使用了参数共享的方法，模型的参数量大大减小。
![](https://pic2.zhimg.com/80/v2-2b5b0ecc4862b83030611ac5e1e0e4a5_hd.jpg)

Bert和ALBert的模型性能比较如下，我们可以看到总体来说，ALBERT的性能优于Bert，并且参数量较小接近5倍，性能也是最优的。
![](https://pic3.zhimg.com/80/v2-fbe79081b8e20aa124501d137de70eca_hd.jpg)

### 总结与问题
本文提出了2种模型参数压缩的方法，即embedding矩阵分解法以及层参数共享法，并通过实验证明词方法的有效性；
同时本文也提出了一个新的预训练任务，SOP，是一个比NSP任务更难并且学习到的东西更多的任务，可以学习到句子内部的顺序。
通过实验证明了这3个创新方案的有效性，目前排在GLUE排行榜第一名。
几个思考问题如下：

论文提及到下游任务性能提高，并且参数量大大减小，训练速度也极大提高，推断速度是不是也是同步的提高，这是需要实验证明；
论文采用矩阵分解来将大矩阵变成两个小矩阵，是不是也会带来损失；

## 参考文献
[ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)
