# 预训练语言模型发展

---

[TOC]

![预训练语言模型](https://tva1.sinaimg.cn/large/008eGmZEgy1gocwwqle8pj31960eagpi.jpg)

## 1. 预训练语言模型诞生

### 1. AR 与 AE 语言模型
- AR 语言模型：指的是，依据前面（或后面）出现的 tokens 来预测当前时刻的 token， 代表有 ELMO， GPT 等

- AE 语言模型：通过**上下文信息**来预测被 mask 的 token， 代表有 BERT , Word2Vec(CBOW)

二者有着它们各自的优缺点：

- AR 语言模型：

  > - **缺点：**它只能利用单向语义而不能同时利用上下文信息。 ELMO 通过双向都做AR 模型，然后进行拼接，但从结果来看，效果并不是太好。
  > - **优点：** 对生成模型友好，天然符合生成式任务的生成过程。这也是为什么 GPT 能够编故事的原因。

- AE 语言模型：

  > - **缺点：** 由于训练中采用了 [MASK] 标记，导致预训练与微调阶段不一致的问题。 此外对于生成式问题， AE 模型也显得捉襟见肘，这也是目前 BERT 为数不多实现大的突破的领域。
  > - **优点：** 能够很好的编码上下文语义信息， 在自然语言理解相关的下游任务上表现突出。

### 2.  Feature-base pre-training：Word Embedding 到 ELMO

考虑到词向量不能解决词的多义性问题，在 ELMO 之前，我们往往采用双向 LSTM 来减轻这种问题，但这毕竟治标不治本，对于大数据集好说， 深层双向 LSTM 的确能够很好的缓解这种问题，但对于小数据集，往往没啥效果。

为了解决这种多义性问题，ELMO 在训练语言模型时采用双向 LSTM 。 不同层的 LSTM 能够把握不同粒度和层级的信息，比如浅层的 LSTM 把握的是单词特征， 中层的 LSTM 把握 句法 特征， 深层的 LSTM 把握语义特征， 对于不同的任务来说， 不同的特征起到了不同的作用。 

ELMO 在迁移到下游任务时，会将不同层的特征采用**加权求和**的方式来获得每个词的最终表示。

缺点也十分明显：

- **LSTM 特征抽取能力远弱于 Transformer ， 并行性差**
- **拼接方式双向融合特征融合能力偏弱**

### 3.  Fine-tuning pretraining：  GPT 的诞生

GPT 虽然不是第一个预训练语言模型，但它的出现更具**开创意义**。其特点很明显：

- 采用**单向 Transformer** 作为特征抽取器
- 采用二阶段： 预训练 + 微调  来适配下游任务

GPT 1.0 与 GPT 2.0 的出现说明了一下几点：

- 高质量，大规模的预训练数据集是提升性能的根本
- 深层的 Transformer 模型具有更强的表示能力

至少，从目前为止， 业界还没有探索到数据与模型的极限，即仅仅堆数据，加深模型这条路，还没有走完。

### 4. 预训练新时代：BERT

GPT 虽然很强，但由于其基于 AR 模型且目前很多排行榜都是基于**自然语言理解**的，因此， GPT 在这方面无法与 BERT 的表现相抗衡。但 GPT 在生成方面是 BERT 无法比拟的

BERT 主要分为两大部分： **Masked LM** 与 **NSP** (Next Sentence Prediction)。

BERT 由于其采用 AE 模型，MASK 操作所带来的缺陷依旧存在：

- 预训练与微调阶段不匹配的问题，这点 BERT 提供了一个策略来减轻该问题
- Mask 掉的  token 之间关系被忽略的问题

此外，由于数据量，模型都十分大，如果每次只 mask 一个token，那么整个训练过程将变得极为漫长， 文章采用 mask 15% 的操作，是一个经验性的选择，是对模型训练效果与训练时长做出了一个权衡。

至于 NSP 任务，事实证明其在句子关系上的确起到了一定的作用，对于某些任务的确有帮助，但也有文章指出，其实用处不大，这点后面会详细讨论。


# 常见面题QA
## 为什么要进行预训练
* 在庞大的无标注数据上进行预训练可以获取更通用的语言表示，并有利于下游任务；
* 为模型提供了一个更好的初始化参数，在目标任务上具备更好的泛化性能、并加速收敛；
* 是一种有效的正则化手段，避免在小数据集上过拟合（一个随机初始化的深层模型容易对小数据集过拟合）；







