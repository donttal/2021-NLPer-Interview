# XLnet论文阅读笔记

## 改进
除了相比Bert增加了训练集之外，XLnet也在模型设计上有较大的改进，比如引入了新的优化目标Permutation Language Modeling（PLM），使用了双流自注意力机制（Two-Stream Self Attention, TSSA）和与之匹配的神奇Mask技巧。

## 简单回顾：BERT与语言模型
相比ELMo使用LSTM，BERT使用了最新的Transformers[5]结构作为backbone，而且能够对下游任务进行fine-tune；而相比GPT的单向Transformer，BERT使用了双向Transformer结构，更有利于提取上下文信息。

在训练方法上，不同于传统的单向语言模型（Language Model, LM）训练，BERT使用了两种方法：Masked LM（MLM）和Next Sentence Prediction（NSP）。对于MLM，BERT的做法是：对输入的句子，随机选取15%的Token，其中的80% 被替换为[MASK]标记，其中的10% 被随机替换为其他Token，另外的10% 保持不变。MLM训练的目标就是要去预测标记为[MASK]的原来的Token。对NSP，BERT把两个句子拼接后送入模型，作为一个二分类任务进行优化。

注意到，BERT的MLM方法和语言模型（LM）不同。LM是对一个句子，单方向地一个接一个地进行预测，这相当于最大化概率$p(x)=\prod_{i=1}^{n} p\left(x_{i} | x_{<i}\right)$，而BERT和DAE（denoising auto-encoder）更为接近。DAE就是在输入端加入噪声，通过模型后输出得到加噪声前的输入，即优化$p(x | \tilde{x})$。

从这个角度讲，BERT中的MLM就是对原输入 [公式] 加入的一种噪声，BERT做的就是“去噪”。基于这种方法，BERT一方面可以更好地结合上下文信息，训练出更好的上下文相关性；另一方面，也使得模型更加robust。相反，LM只能从一个方向去考虑上文信息，而不能充分利用上下文的关系，这是LM的一个不足之处。

**但是，从生成类任务（如MT）的角度来讲，LM相比之下更加自然和简单。生成类任务要求我们逐个生成Token，这和LM的训练机制不谋而合，而BERT由于训练过程和生成过程的不一致，导致其在生成类任务上的效果不佳。**

## XLnet提出背景：BERT与LM缺点
XLnet的提出是基于BERT和LM的固有缺点而言的。在上节我们已经提到了BERT的一个缺点：预训练与微调的模式不匹配问题。因为BERT是DAE模式的，而生成类任务是LM（或Autoregressive, AR）模式的，这就在训练和微调两个阶段产生了不一致。实际上，BERT的MLM方法就有缓解这个问题的目的，但不能从根本上解决。这是一点。

另一方面，BERT句子中所有的[MASK]是相互独立的。注意比较上节中的BERT和LM各自的优化目标。可以看到，LM的优化是基于概率的链式法则，也就是考虑了上文的相关性；而BERT的优化是基于上下文的“复原”，忽略了各个[MASK]之间的相关性。

**第三，对LM而言，它只能考虑单方向的文本信息。LM只能从一个方向，要么从左到右，要么从右到左进行优化，而BERT考虑的是上下文的信息。**

从上面的三点来看，BERT和LM都有其不足之处，那么有没有一个办法结合两者的优点且避免其缺点呢？这就是下面XLnet要做的事情。

## 巧妙的置换与神奇的Mask
使用Permutation Language Modeling（PLM），即对输入顺序本身进行置换。

给定长度为$T$的序列$x$，它的原下标为$[1,2, \ldots, T]$，其有置换的集合为一个全排列$Z_{T}$，这样我们就可以转为优化下面的目标：
$$\max _{\theta} E_{z \sim \mathcal{Z}_{\mathcal{T}}}\left[\sum_{t=1}^{T} \log p_{\theta}\left(x_{z_{t}} | \mathrm{x}_{\mathrm{z}<\mathrm{t}}\right)\right]$$

注意到这里 [公式] 始终是 [公式] 的一个排列，输入到模型的Token顺序并没有改变，所以模型的参数在不同的排列下也是保持不变的，这样一来，如果我们在当前排列 [公式] 上进行类似于LM的操作，理论上，对 [公式] ，始终可以看到所有的 [公式] 。换句话说，这就相当于在考虑 [公式] 的时候考虑了它的上下文。并且，这样的置换操作可以很自然地归入AR框架，这就结合了BERT和LM的优点。

还是举“我爱北京”的例子。现在其下标为（我，1），（爱，2），（北，3），（京，4）。如果我们置换得到的是2->4->3->1，那么训练的顺序就为“爱->京->北->我”,也就是按照这个顺序去执行类似LM的操作。可以看到，原本LM训练时“北”看不到“京”，现在可以看到了；原来BERT中“北”和“京”相互独立，现在预测“京”可以基于“北”的信息，而且整个训练过程也完全符合LM的模式。这样一来，就完美解决了上面的三个问题。

那么，我们该怎么把LM和预训练模型相结合呢？XLnet提出的双流自注意力机制（Two-Stream Self-Attention，TSSA）就是上述方法的具体操作。这个机制的核心就是BERT所谓的掩码Mask。

首先要再次强调，输入到模型的文本顺序保持不变，比如，在上述的置换下，输入到模型的仍然是“我爱北京”而不是“爱京北我”，实现置换的方法就是TSSA。具体来说，不同于之前类似BERT的模型，XLnet使用了两组不同的表征单元，XLnet分别称为内容表征单元（content representation）和询问表征单元（query representation）。内容单元和之前一样，是对上文信息的表示，需要涉及当前Token。而询问单元只能访问当前Token的位置信息而不能访问当前Token的内容信息。这两个信息流通过自注意力机制向上传递，在最后一层使用询问单元去输出。

输出的是什么呢？输出的就是输入的内容。比如，输入的是“我爱北京”，输出的还是“我爱北京”。但是问题来了，不是说我们已经对输入顺序置换了吗，怎么输出还能是原来的输入且保持顺序不变呢？而且LM的处理模式好像也没有体现。这就是XLnet神奇的地方，它巧妙地使用了Mask达到上面两个目的。

我们以下图为例进行阐释，其中，1234对应的就是“我爱北京”，并且置换为3241，也即“北爱京我”。因为句子的长度为4，所以我们有两个 [公式] 的Mask矩阵，分别对应了内容流和询问流。如图(c)，两个矩阵都是从上到下行标为1234，从左到右列标为1234，再次提醒，这里的1234分别和“我爱北京”是对应的。

我们来看上面的内容流矩阵。第一行对应的是“我”，“我”在这个置换顺序中是最后一个（3241），所以它可以看到自己及自己之前的所有Token，体现在矩阵上，把所有四列都设为1（图中标为红色）。再看第二行，对应的是“爱”，在置换中的顺序是第二个，所以可以看到自己和3，也即“北”，在矩阵对应的位置标记。同理，对“北”和“京”也是一样的操作。而询问流矩阵只需要把内容流矩阵的对角线抹去即可（因为询问流看不到自己的信息）。这两个矩阵的使用和Transformer一样，只需要对相应的自注意力矩阵做Hadamard乘积即可。

可以看到，通过这种巧妙的Mask方法，XLnet完美结合了LM的优点和上下文的相关性，并且比起LSTM，还具有可并行的优势。

实际上，这里的Mask和BERT的Mask在本质上是一样的，都可以看作是一种加噪的过程。不同的是，XLnet的Mask利用率更高。这是因为，对同一个句子，我们可以有不同的置换，而且对同一个置换，我们可以像LM那样遍历整个句子，相当于遍历了所有可能的上下文情况。

![](https://pic2.zhimg.com/80/v2-e48193002b5cba4224463d9469a3da85_hd.jpg)

XLnet还使用了部分预测（Partial Prediction）的方法。因为LM是从第一个Token预测到最后一个Token，在预测的起始阶段，上文信息很少而不足以支持Token的预测，这样可能会对分布产生误导，从而使得模型收敛变慢。为此，XLnet只预测后面一部分的Token，而把前面的所有Token都当作上下文。具体来说，对长度为$T$的句子，我们选取一个超参数$K$，使得后面$1/K$的Token用来预测，前面的$1-1/k$的Token用作上下文。注意， $K$越大，上下文越多，模型预测得就越精确。

此外，XLnet还使用了Transformer-XL作为Backbone，也使用了T-XL的相对位置编码，所以相比BERT，XLnet对长文档的支持更加有效

## 实验
![](https://pic2.zhimg.com/80/v2-94c380f2d023da08f6eea01f9a757d55_hd.jpg)

## 参看网站
[知乎链接](https://zhuanlan.zhihu.com/p/71759544)