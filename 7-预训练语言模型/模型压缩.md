# 模型压缩
[网址](https://www.cnblogs.com/shona/p/12935591.html)

[toc]

## 量化
用FP16或者INT8代替模型参数，一是占用了更少内存，二是接近成倍地提升了计算速度。目前FP16已经很常用了，INT8由于涉及到更多的精度损失还没普及。

## 低轶近似／权重共享
低轶近似是用两个更小的矩阵相乘代替一个大矩阵，权重共享是12层transformer共享相同参数。这两种方法都在**ALBERT**中应用了，对速度基本没有提升，主要是减少了内存占用。但通过ALBRET方式预训练出来的Transformer理论上比BERT中的层更通用，可以直接拿来初始化浅层transformer模型，相当于提升了速度。

## 剪枝
通过去掉模型的一部分减少运算。最细粒度为权重剪枝，即将某个连接权重置为0，得到稀疏矩阵；其次为神经元剪枝，去掉矩阵中的一个vector；模型层面则为结构性剪枝，可以是去掉attention、FFN或整个层，典型的工作是LayerDrop。这两种方法都是同时对速度和内存进行优化。

## 蒸馏
训练时让小模型学习大模型的泛化能力，预测时只是用小模型。比较有名的工作是DistillBERT[2]和TinyBERT[3]。
实际工作中，减少BERT层数+蒸馏是一种常见且有效的提速做法。但由于不同任务对速度的要求不一样，可能任务A可以用6层的BERT，任务B就只能用3层的，因此每次都要花费不少时间对小模型进行调参蒸馏。

# DynaBERT
《DynaBERT: Dynamic BERT with Adaptive Width and Depth》[4]。论文中作者提出了新的训练算法，**同时对不同尺寸的子网络进行训练，通过该方法训练后可以在推理阶段直接对模型裁剪。**依靠新的训练算法，本文在效果上超越了众多压缩模型，比如DistillBERT、TinyBERT以及LayerDrop后的模型。

## 原理
论文对于BERT的压缩流程是这样的：
训练时，对宽度和深度进行裁剪，训练不同的子网络
推理时，根据速度需要直接裁剪，用裁剪后的子网络进行预测
想法其实很简单，但如何能保证更好的效果呢？这就要看炼丹功力了 (..•˘_˘•..)，请听我下面道来～
整体的训练分为两个阶段，先进行宽度自适应训练，再进行宽度+深度自适应训练。

### 宽度自适应 Adaptive Width
宽度自适应的训练流程是：
得到适合裁剪的teacher模型，并用它初始化student模型
裁剪得到不同尺寸的子网络作为student模型，对teacher进行蒸馏
最重要的就是如何得到适合裁剪的teacher。先说一下宽度的定义和剪枝方法。Transformer中主要有Multi-head Self-attention(MHA)和Feed Forward Network(FFN)两个模块，为了简化，作者用注意力头的个数和intermediate层神经元的个数来定义MHA和FFN的宽度，并使用同一个缩放系数来剪枝，剪枝后注意力头减小到个，intermediate层神经元减少到个。

在MHA中，我们认为不同的head抽取到了不同的特征，因此每个head的作用和权重肯定也是不同的，intermediate中的神经元连接也是。如果直接按照粗暴裁剪的话，大概率会丢失重要的信息，因此作者想到了一种方法，对head和神经元进行排序，每次剪枝掉不重要的部分，并称这种方法为Netword Rewiring。
对于重要程度的计算参考了论文[5]，核心思想是计算去掉head之前和之后的loss变化，变化越大则越重要。

## 深度自适应 Adaptive Depth
训好了width-adaptive的模型之后，就可以训自适应深度的了。浅层BERT模型的优化其实比较成熟了，主要的技巧就是蒸馏。作者直接使用训好的作为teacher，蒸馏裁剪深度后的小版本BERT。

对于深度，系数md = [1.0, 0,75, 0,5]，设层的深度为[1,12]，作者根据mod(d+1, 1/md) ≡ 0去掉深度为d的层。之所以取d+1是因为研究表明最后一层比较重要[Minilm]。作者根据去掉深度为d的层。之所以取是因为研究表明最后一层比较重要[6]。


# ALBERT
## 三大创新

### 1. Factorized embedding parameterization

该 Trick 本质上就是一个**低秩分解**的操作，其通过对Embedding 部分降维来达到降低参数的作用。在最初的BERT中，以Base为例，Embedding层的维度与隐层的维度一样都是768，但是我们知道，对于词的分布式表示，往往并不需要这么高的维度，比如在Word2Vec时代就多采用50或300这样的维度。那么一个很简单的思想就是，通过将Embedding部分分解来达到降低参数量的作用，其以公式表示如下：
$$
O(V \times H) \to O(V \times E + E \times H)
$$

- V：词表大小；H：隐层维度；E：词向量维度

我们以 BERT-Base 为例，Base中的Hidden size 为768， 词表大小为3w，此时的参数量为：768 * 3w = 23040000。 如果将 Embedding 的维度改为 128，那么此时Embedding层的参数量为： 128 * 3w + 128 * 768 = 3938304。二者的差为19101696，大约为19M。我们看到，其实Embedding参数量从原来的23M变为了现在的4M，似乎变化特别大，然而当我们放到全局来看的话，BERT-Base的参数量在110M，降低19M也不能产生什么革命性的变化。因此，可以说**Embedding层的因式分解其实并不是降低参数量的主要手段。**

注意，我在这里特意忽略了Position Embedding的那部分参数量， 主要是考虑到512相对于3W显得有点微不足道。


### 2. Cross-layer parameter sharing

该Trick本质上就是对**参数共享机制**在Transformer内的探讨。在Transfor中有两大主要的组件：FFN与多头注意力机制。ALBERT主要是对这两大组件的共享机制进行探讨。

论文里采用了四种方式： **all-shared，shared-attention，shared-FFN以及 not-shared。**我们首选关注一下参数量，not-shared与all-shared的参数量相差极为明显，因此可以得出共享机制才是参数量大幅减少的根本原因。然后，我们看到，只共享Attention参数能够获得参数量与性能的权衡。最后，很明显的就是，随着层数的加深，基于共享机制的 ALBERT 参数量与BERT参数量相比下降的更加明显。

此外，文章还说道，通过共享机制，能够帮助模型稳定网络的参数。这点是通过L2距离与 cosine similarity 得出的，俺也不太懂，感兴趣的可以找其他文章看看：

![3](..\img\ALBERT\3.PNG)

### 3. SOP 替代 NSP

SOP 全称为 Sentence Order Prediction，其用来取代 NSP 在 BERT 中的作用，毕竟一些实验表示NSP非但没有作用，反而会对模型带来一些损害。SOP的方式与NSP相似，其也是判断第二句话是不是第一句话的下一句，但对于负例来说，SOP并不从不相关的句子中生成，而是将原来连续的两句话翻转形成负例。

很明显的就可以看出，SOP的设计明显比NSP更加巧妙，毕竟NSP任务的确比较简单，不相关句子的学习不要太容易了。论文也比较了二者：

![4](..\img\ALBERT\4.PNG)

## BERT vs ALBERT

### 1. 从参数量级上看

![5](..\img\ALBERT\5.PNG)

首先，参数量级上的对比如上表所示，十分明显。这里需要提到的是ALBERT-xxlarge，它只有12层，但是隐层维度高达4096，这是考虑到深层网络的计算量问题，其本质上是一个浅而宽的网络。

### 2. 从模型表现上看

![6](..\img\ALBERT\6.PNG)

首先，我们看到 ALBERT-xxlarge的表现完全超过了BERT-large的表现，但是BERT-large的速度是要比ALBERT-xxlarge快3倍左右的。

其次，BERT-xlarge的表现反而变差，这点在[2]中有详细探讨，先略过不表。

## Tricks

其实，最近很多预训练语言模型文章中都相当程度上提到了调参以及数据处理等Trick的重要性，可惜我等没有资源去训，本来想在结尾将本文的Trick都提一下，但由于无法形成对比，因此作罢。等过段时间将各大预训练语言模型中的Trick‘汇总一下再说吧。

## 最后

其实，从模型创新的角度来看，ALBERT其实并没有什么很大的创新，如果是一个轻量级的任务，相信这种模型压缩的方式早就做烂了。可惜的是，计算资源限制了绝大多数的实验室和公司，只能看头部公司笑傲风云。

此外，这篇文章本质上就是一个工程性质的文章，我觉得其中的一些Trick都十分的有借鉴意义，等我有时间再搞搞吧。

## Reference

[如何看待瘦身成功版BERT——ALBERT？](<https://www.zhihu.com/question/347898375>)

[1] ALBERT: A Lite BERT for Self-supervised Learning of Language Representations

[2] T5 - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer